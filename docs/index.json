[
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/comparison/tutorials/",
	"title": "Dataset Used",
	"tags": [],
	"description": "",
	"content": " Dataset Used Configuration bash pre-install.sh  End-to-end benchmarking For end-to-end benchmarking, a series of deep learning frameworks are required. Please refer to the Frameworks for framework information and installation.\nMicro Benchmark The micro benchmark is based on the DeepBench (By Baidu Research), primarily covering the deep learning components, such as Dense Matrix Multiplies, Convolutions, Recurrent Layers, and All Reduce. Please refer to the microbench folder for more information.\nWorkloads MNIST CIFAR-10 CIFAR-100 System Configuration:  Operating System: Ubuntu 16.04 Memory: 10 GB GPU: Nvidia GTX 1080 Ti (11GB) CPU: E5-1620 (3.6GHz) Hard Drive: 256 GB  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/comparison/tutorials/benchmarking_on_mnist/",
	"title": "MNIST",
	"tags": [],
	"description": "",
	"content": " Benchmarking on MNIST: The following mentioned model definition files are under the folder: models/mnist/ .\nPre-setting: DLBENCH_ROOT=\u0026quot;path to the root directory of this benchmark\u0026quot;  TensorFlow: TensorFlow uses a variant of LeNet and its network structure is shown as follows:   TensorFlow default model   Run TensorFlow with its default MNIST setting:\ncd $DLBENCH_ROOT/models/mnist/tensorflow/ python mnist_deep.py  The Training Time, Testing Time and Accuracy will appear after completion.\nCaffe: Similarly, the NN network structure of Caffe is shown as follows:\nIt is also a variant of LeNet.\nRun Caffe with its default MNIST setting:\ncd $DLBENCH_ROOT/models/mnist/caffe caffe train -solver lenet_solver.prototxt \u0026gt; log.txt 2\u0026gt;\u0026amp;1  The Training Time, Testing Time and Accuracy can be extracted from the log.txt file.\nTorch: cd $DLBENCH_ROOT/models/mnist/torch  Run on CPU:\nth train-on-mnist.lua  Theano: cd $DLBENCH_ROOT/models/mnist/theano THEANO_FLAGS=device=cpu python convolutional_mlp.py  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/comparison/tutorials/benchmarking_on_cifar10/",
	"title": "CIFAR-10",
	"tags": [],
	"description": "",
	"content": " Benchmarking on CIFAR-10: The following mentioned model definition files are under the folder: models/cifar10/ .\nPre-setting: DLBENCH_ROOT=\u0026quot;path to the root directory of this benchmark\u0026quot;  TensorFlow: Run TensorFlow with its default MNIST setting:\ncd $DLBENCH_ROOT/models/cifar10/tensorflow/ python cifar10_train.py \u0026gt; train_log.txt 2\u0026gt;\u0026amp;1  After the completion of training, run the following command to test the tranined model:\npython cifar10_eval.py \u0026gt; test_log.txt 2\u0026gt;\u0026amp;1  The Accuracy will appear after completion of cifar10_eval.py. And the Training Time and Testing Time can be extracted from the train_log.txt and test_log.txt.\nCaffe: Similarly, the NN network structure of Caffe is shown as follows:\nRun Caffe with its default setting:\ncd $DLBENCH_ROOT/models/cifar10/caffe ./train_quick.sh \u0026gt; log.txt 2\u0026gt;\u0026amp;1  The Training Time, Testing Time and Accuracy can be extracted from the log.txt file.\nTorch: cd $DLBENCH_ROOT/models/cifar10/torch  Run on CPU:\nth train-on-cifar-10.lua  Theano: Note: The implementation for Theano on CIFAR-10 derived from Reslab Theano tutorial (10 February 2015)\ncd $DLBENCH_ROOT/models/cifar10/theano THEANO_FLAGS=device=cpu python convolutional_mlp.py  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/",
	"title": "Datasets",
	"tags": [],
	"description": "",
	"content": " Datasets Overview Datasets in Various Formats    Datasets\\Format RAW CSV LMDB Other     MNIST RAW CSV LMDB -   CIFAR-10 RAW - LMDB -   CIFAR-100 RAW - LMDB -   AT\u0026amp;T Face RAW CSV - RDS    "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/comparison/tutorials/benchmarking_on_cifar100/",
	"title": "CIFAR-100",
	"tags": [],
	"description": "",
	"content": " Benchmarking on CIFAR-100: The following mentioned model definition files are under the folder: models/cifar100/ .\nPre-setting: DLBENCH_ROOT=\u0026quot;path to the root directory of this benchmark\u0026quot;  TensorFlow: Run TensorFlow with its default MNIST setting:\ncd $DLBENCH_ROOT/models/cifar100/tensorflow/ python cifar100_train.py \u0026gt; train_log.txt 2\u0026gt;\u0026amp;1  After the completion of training, run the following command to test the tranined model:\npython cifar100_eval.py \u0026gt; test_log.txt 2\u0026gt;\u0026amp;1  The Accuracy will appear after completion of cifar100_eval.py. And the Training Time and Testing Time can be extracted from the train_log.txt and test_log.txt.\nCaffe: Similarly, the NN network structure of Caffe is shown as follows:\nRun Caffe with its default setting:\ncd $DLBENCH_ROOT/models/cifar100/caffe ./train_quick.sh \u0026gt; log.txt 2\u0026gt;\u0026amp;1  The Training Time, Testing Time and Accuracy can be extracted from the log.txt file.\nTorch: cd $DLBENCH_ROOT/models/cifar100/torch  Run on CPU:\nth train-on-cifar-100.lua  Theano: Note: The implementation for Theano on CIFAR-100 derived from Reslab Theano tutorial (10 February 2015)\ncd $DLBENCH_ROOT/models/cifar100/theano THEANO_FLAGS=device=cpu python convolutional_mlp.py  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/frameworks/",
	"title": "Frameworks",
	"tags": [],
	"description": "",
	"content": " Deep Learning Frameworks    Frameworks Creator Version Github Hash Tag Numerical Library Platform Written in Line of Codes Interface License Website     TensorFlow Google Brain team 1.3.0 ab0fcac Eigen, CUDA Linux, macOS, Windows C++, Python 1281085 Java, Python, Go, R Apache 2.0 TensorFlow   Caffe Berkeley Vision and Learning Center 1.0.0 c430690 OpenBLAS \u0026amp; CUDA Linux, macOS, Windows C++ 69608 Python, MATLA BSD license Caffe   Torch Ronan Collobert, Koray Kavukcuoglu, Clement Farabet torch7 0219027 optim \u0026amp; CUDA Linux, macOS C, Lua 29750 Lua BSD license Torch   MXNet Apache Software Foundation 1.0.0 ccb08fb OpenBLAS \u0026amp; CUDA Linux, MacOS, Windows, AWS, Raspberry Pi, NVIDIA Jetson Tx2 C++ 216809 Python, Scala, R, Julia, Perl Apache 2.0 MXNet    For more information about framworks: Comparison of deep learning software\nNote: Line of Codes is given by CLOC, LOC.\nThe installation scripts are at the root of the Github project.\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/mnist_datasets/",
	"title": "MNIST",
	"tags": [],
	"description": "",
	"content": " MNIST Dataset The MNIST database of handwritten digits\nDownload Raw Dataset\nDataset Statistics  Color: Grey-scale Sample Size: 28x28  The number of categories of MNIST is 10, that is 0-9, 10 digits.\nThe Number of Samples per Category for MNIST    Category 0 1 2 3 4 5 6 7 8 9 Total     #Training Samples 5,923 6,742 5,958 6,131 5,842 5,421 5,918 6,265 5,851 5,949 60,000   #Testing Samples 980 1,135 1,032 1,010 982 892 958 1,028 974 1,009 10,000    Samples Dataset Usage MNIST in CSV The format is:\nlabel, pix-11, pix-12, pix-13, ...  And the script to generate the CSV file from the original dataset is included in this dataset.\nDownload_MNIST_CSV\nRefer to MNIST in CSV\nTensorFlow: TensorFlow provides a simple method for Python to use the MNIST dataset. @tensorflow_MNIST_For_ML_Beginners\nfrom tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)  Caffe: Caffe will download and convert the MNIST dataset to LMDB format throught the scripts. @caffe_Training_LeNet_on_MNIST_with_Caffe\nexport CAFFE_ROOT='path_to_caffe_root_folder' cd $CAFFE_ROOT ./data/mnist/get_mnist.sh ./examples/mnist/create_mnist.sh  Download MNIST for Caffe\nTorch Torch will download MNIST automatically by executing:\nth train-on-mnist.lua  General tools for Python: python-mnist\nmnist: Python utilities to download and parse the MNIST dataset\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/cifar-10_datasets/",
	"title": "CIFAR-10",
	"tags": [],
	"description": "",
	"content": " CIFAR-10 dataset The CIFAR-10 dataset\nDataset Statistics  Color: RGB Sample Size: 32x32  The number of categories of CIFAR-10 is 10, that is airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\nThe Number of Samples per Category for CIFAR-10    Category airplane automobile bird cat deer dog frog horse ship truck Total     #Training Samples 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 50,000    #Testing Samples 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 10,000    Samples Dataset Usage TensorFlow: TensorFlow @TensorFlow_Convolutional_Neural_Networks\nCaffe: export CAFFE_ROOT='path_to_caffe_root_folder' cd $CAFFE_ROOT ./data/cifar10/get_cifar10.sh ./examples/cifar10/create_cifar10.sh  Caffe @caffe_Alex\u0026rsquo;s_CIFAR-10_tutorial_Caffe_style\nDowload CIFAR-10 for Caffe\nTorch: th train-on-cifar-10.lua  General tools for Python Official Python function: def unpickle(file): import cPickle with open(file, 'rb') as fo: dict = cPickle.load(fo) return dict  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/comparison/",
	"title": "Comparison of DL Frameworks",
	"tags": [],
	"description": "",
	"content": "This section will present benchmarking results and trained models.\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/comparison/mnist_comparison/",
	"title": "Comparison on MNIST",
	"tags": [],
	"description": "",
	"content": "The benchmarking results are as following figure shows:\n      google.charts.load('visualization', '1', {'package':['corechart', 'controls']}); function drawChart() { $.get(\"http:\\/\\/YanzhaoWu.github.io\\/GTDLBench\\/data\\/MNISTServer1.csv\", function(csvString) { var tmpData = $.csv.toArrays(csvString, {onParseValue: $.csv.hooks.castToScala}); var arrayData = []; var arrayData1 = []; var arrayData2 = []; for (var i = 0; i  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/info/",
	"title": "About",
	"tags": [],
	"description": "",
	"content": "Please cite the following papers:\n@INPROCEEDINGS{GTDLBenchICDCS, author={{Liu}, Ling and {Wu}, Yanzhao and {Wei}, Wenqi and {Cao}, Wenqi and {Sahin}, Semih and {Zhang}, Qi}, booktitle={2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)}, title=\u0026quot;{Benchmarking Deep Learning Frameworks: Design Considerations, Metrics and Beyond}\u0026quot;, year={2018}, pages={1258-1269}, doi={10.1109/ICDCS.2018.00125}, ISSN={2575-8411}, month={July}, } @ARTICLE{GTDLBencharXiv, author = {{Wu}, Yanzhao and {Liu}, Ling and {Pu}, Calton and {Cao}, Wenqi and {Sahin}, Semih and {Wei}, Wenqi and {Zhang}, Qi}, title = \u0026quot;{A Comparative Measurement Study of Deep Learning as a Service Framework}\u0026quot;, journal = {arXiv e-prints}, keywords = {Computer Science - Performance, Computer Science - Machine Learning}, year = 2018, month = Oct, eid = {arXiv:1810.12210}, pages = {arXiv:1810.12210}, archivePrefix = {arXiv}, eprint = {1810.12210}, primaryClass = {cs.PF}, adsurl = {https://ui.adsabs.harvard.edu/\\#abs/2018arXiv181012210W}, adsnote = {Provided by the SAO/NASA Astrophysics Data System} } @INPROCEEDINGS{GTDLBenchBigData, author={{Wu}, Yanzhao and and {Cao}, Wenqi and {Sahin}, Semih and {Liu}, Ling}, booktitle={2018 IEEE 38th International Conference on Big Data}, title=\u0026quot;{Experimental Characterizations and Analysis of Deep Learning Frameworks}\u0026quot;, year={2018}, month={December}, }  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/cifar-100_datasets/",
	"title": "CIFAR-100",
	"tags": [],
	"description": "",
	"content": " CIFAR-100 dataset The CIFAR-100 dataset\nDataset Statistics  Color: RGB Sample Size: 32x32  This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are roughly grouped into 20 superclasses. Each image comes with a \u0026ldquo;fine\u0026rdquo; label (the class to which it belongs) and a \u0026ldquo;coarse\u0026rdquo; label (the superclass to which it belongs). Here is the list of classes in the CIFAR-100:\n   Superclass Classes     aquatic mammals beaver, dolphin, otter, seal, whale   fish aquarium fish, flatfish, ray, shark, trout   flowers orchids, poppies, roses, sunflowers, tulips   food containers bottles, bowls, cans, cups, plates   fruit and vegetables apples, mushrooms, oranges, pears, sweet peppers   household electrical devices clock, computer keyboard, lamp, telephone, television   household furniture bed, chair, couch, table, wardrobe   insects bee, beetle, butterfly, caterpillar, cockroach   large carnivores bear, leopard, lion, tiger, wolf   large man-made outdoor things bridge, castle, house, road, skyscraper   large natural outdoor scenes cloud, forest, mountain, plain, sea   large omnivores and herbivores camel, cattle, chimpanzee, elephant, kangaroo   medium-sized mammals fox, porcupine, possum, raccoon, skunk   non-insect invertebrates crab, lobster, snail, spider, worm   people baby, boy, girl, man, woman   reptiles crocodile, dinosaur, lizard, snake, turtle   small mammals hamster, mouse, rabbit, shrew, squirrel   trees maple, oak, palm, pine, willow   vehicles 1 bicycle, bus, motorcycle, pickup truck, train   vehicles 2 lawn-mower, rocket, streetcar, tank, tractor    The Number of Samples per Category for MNIST    Category Total per Category     #Training Samples 50,000 500   #Testing Samples 10,000 1000    Caffe: Convert the raw data into the LMDB format:  Change directory to datasets:  cd tutorials/datasets/  Download CIFAR-100 python version  wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz  Extract files  tar -xf cifar-100-python.tar.gz \u0026amp;\u0026amp; rm -f cifar-100-python.tar.gz  Generate LMDB files (Install missing libraries for Python)\npython convert_cifar100_lmdb.py  Use CIFAR-100 in LMDB format: Add the following data layer definition into the network prototxt file to use this CIFAR-100 dataset.\n  layer { name: \u0026quot;cifar100\u0026quot; type: \u0026quot;Data\u0026quot; top: \u0026quot;data\u0026quot; top: \u0026quot;label\u0026quot; include { phase: TRAIN } transform_param { mean_file: \u0026quot;../mean.binaryproto\u0026quot; } data_param { source: \u0026quot;../cifar100_train_lmdb\u0026quot; batch_size: 128 backend: LMDB } } layer { name: \u0026quot;cifar100\u0026quot; type: \u0026quot;Data\u0026quot; top: \u0026quot;data\u0026quot; top: \u0026quot;label\u0026quot; include { phase: TEST } transform_param { mean_file: \u0026quot;../mean.binaryproto\u0026quot; } data_param { source: \u0026quot;../cifar100_test_lmdb\u0026quot; batch_size: 100 backend: LMDB } }  Download CIFAR-100 for Caffe\nGeneral tools for Python Modify the input file cifar10_input.py described in TensorFlow @TensorFlow_Convolutional_Neural_Networks to support the CIFAR-100 dataset.\nOfficial Python function: def unpickle(file): import cPickle with open(file, 'rb') as fo: dict = cPickle.load(fo) return dict  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/comparison/cifar10_comparison/",
	"title": "Comparison on CIFAR-10",
	"tags": [],
	"description": "",
	"content": "The benchmarking results are as following figure shows:\n      google.charts.load('visualization', '1', {'package':['corechart', 'controls']}); function drawChart() { $.get(\"http:\\/\\/YanzhaoWu.github.io\\/GTDLBench\\/data\\/CIFAR10Server1.csv\", function(csvString) { var tmpData = $.csv.toArrays(csvString, {onParseValue: $.csv.hooks.castToScala}); var arrayData = []; var arrayData1 = []; var arrayData2 = []; for (var i = 0; i  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/att_face_dataset/",
	"title": "Faces (AT&amp;T)",
	"tags": [],
	"description": "",
	"content": " The Database of Faces (AT\u0026amp;T) The Database of Faces\nDataset Statistics  Color: Grey-scale Sample Size: 92x112 #Samples: 400 Dataset Size: 4.5 MB (compressed in .tar.z)  The original files are in PGM format, and can conveniently be viewed on UNIX \u0026trade; systems using the \u0026lsquo;xv\u0026rsquo; program. The size of each image is 92x112 pixels, with 256 grey levels per pixel. The images are organised in 40 directories (one for each subject), which have names of the form sX, where X indicates the subject number (between 1 and 40). In each of these directories, there are ten different images of that subject, which have names of the form Y.pgm, where Y is the image number for that subject (between 1 and 10).\nThe AT\u0026amp;T face dataset, \u0026ldquo;(formerly \u0026lsquo;The ORL Database of Faces\u0026rsquo;), contains a set of face images taken between April 1992 and April 1994 at the lab. The database was used in the context of a face recognition project carried out in collaboration with the Speech, Vision and Robotics Group of the Cambridge University Engineering Department.\u0026rdquo;\n\u0026ldquo;There are 10 different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). A preview image of the Database of Faces is available.\u0026rdquo;\nFull Dataset Dataset Usage TensorFlow Face Recognition using Deep Learning and TensorFlow Framework\nRun the following commands:\ngit clone https://github.com/alpha-13/Face-Recognition.git cd Face-Recognition python Face\\ Recognition\\ TensorFlow.py  PyTorch Facial Similarity with Siamese Networks in Pytorch\nRun the following commands to install the dependencies:\ngit clone https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch.git cd Facial-Similarity-with-Siamese-Networks-in-Pytorch pip install -r requirements.txt  Converting pgm files to png: (Need to install imagemagick)\ncd \u0026quot;Root directory of the images\u0026quot; find -name \u0026quot;*pgm\u0026quot; | xargs -I {} convert {} {}.png jupyter nbconvert --execute Siamese-networks-medium.ipynb  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/caltech101_datasets/",
	"title": "CALTECH101",
	"tags": [],
	"description": "",
	"content": " CALTECH101 The CALTECH101 dataset\nCaltech-101 contains a total of 9,146 images, split between 101 distinct object categories (faces, watches, ants, pianos, etc.) and a background category. This dataset contains 102 folders, the BACKGROUND_Google (the background category) can be removed, and users may use the left 101 categoies.\nDataset Statistics  Color: RGB Sample size: Roughtly 300x200 Dataset size: 1.2 GB  Overall, the dataset consists of pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. The size of each image is roughly 300x200 pixels. Almost all images are annotated with the following information: a bounding box of the object, and a carefully traced silhouette of the objects by a human subject.\nThe Number of Samples per Category for Caltech 101 TODO\nSamples Dataset Usage Theano ini_caltech101: Getting the Code To get a local copy of the code, clone it using git:\ngit clone https://github.com/marcuniq/ini_caltech101.git cd ini_caltech101  Make sure you have the bleeding edge version of Theano, or run\npip install --upgrade --no-deps git+git://github.com/Theano/Theano.git  Next, install the package. Use \u0026lsquo;develop\u0026rsquo; instead of \u0026lsquo;install\u0026rsquo; if you consider changing package code\npython setup.py develop  Run train.sh (sets proper theano env flags), which downloads and untars the \u0026lsquo;img-gen-resized\u0026rsquo; dataset, then starts training.\n./train.sh  Keras deeplearn-caltech101: Build on top of VGG19 The dataset is already contained in this project.\npip install -r requirements.txt python2.7 caltech_convnet.py  Deeplearning4j Caltech101Classifier: VGG16 pretrained model. How to run the project:  IntelliJ IDE:\nThis is a maven project. It\u0026rsquo;s developed in IntelliJ. The project can be loaded and run in IntelliJ. When run in IntelliJ, under \u0026ldquo;Run\u0026rdquo;-\u0026gt;\u0026ldquo;Edit Configurations\u0026rdquo;, update following:\n VM options: -Xms8g -Xmx8g Program arguments:   Command line:\nThe Caltech101Classifier-1.0-SNAPSHOT-jar-with-dependencies.jar is under target folder.\nRun Caltech101Classifier-1.0-SNAPSHOT-jar-with-dependencies.jar:\nGo to the jar directory and run:\njava -jar -Xms8g -Xmx8g Caltech101Classifier-1.0-SNAPSHOT-jar-with-dependencies.jar \u0026lt;data path\u0026gt;   Reference:  Images only: L. Fei-Fei, R. Fergus and P. Perona. Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories. IEEE. CVPR 2004, Workshop on Generative-Model Based Vision. 2004\n Images and annotations: L. Fei-Fei, R. Fergus and P. Perona. One-Shot learning of object categories. IEEE Trans. Pattern Recognition and Machine Intelligence. In press.\n  Specific Categories:  trilobite face pagoda tick inlineskate metronome accordion yinyang soccerball spotted cat nautilus grand-piano crayfish headphone hawksbill ferry cougar-face bass ketch lobster pyramid rooster laptop waterlilly wrench strawberry starfish ceilingfan seahorse stapler stop-sign zebra brontosaurus emu snoopy okapi schooner binocular motorbike hedgehog garfield airplane umbrella panda crocodile-head llama windsor-chair car-side pizza minaret dollarbill gerenuk sunflower rhino cougar-body crab ibis helicopter dalmatian scorpion revolver beaver saxophone kangaroo euphonium flamingo flamingo-head elephant cellphone gramophone bonsai lotus cannon wheel-chair dolphin stegosaurus brain menorah chandelier camera ant scissors butterfly wildcat crocodile barrel joshua-tree pigeon watch dragonfly mayfly cup ewer octopus platypus buddha chair anchor mandolin electric-g  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/caltech256_datasets/",
	"title": "CALTECH256",
	"tags": [],
	"description": "",
	"content": " CALTECH256 The CALTECH256 dataset\nDataset Statistics  Color: RGB Sample Size:  Camprison with Caltech-101: The Number of Samples per Category for Caltech-256 Samples Dataset Usage TensorFlow: Git clone https://github.com/yukunchen113/ResnetCNN.git cd RestNetCNN   Use caltech256_bin.py to convet caltech256 images to tfrecord files for faster reading. Use caltech256_input.py input functions to convert input iput functions to batch images and labels. Model.py contains resnet model. Train.py trains the model Evals.py evaluates the model. \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;  Caffe:  Git clone https://github.com/PramuPerera/DeepOneClass.git  Pre-processing  This code is developed targeting pycaffe framework. Please make sure caffe and python 2.7 is installed. Download the code into caffe/examples folder. Download pre-trained models to caffe/models folder. For VGG16 visit : http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel For Alexnet visit : http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel Download reference dataset to caffe/data. We use ImageNet validation set. It can be found at http://www.image-net.org/challenges/LSVRC/2012/nonpub-downloads Download target datasets to caffe/data. For novelty detection we use Caltech 256 : http://www.vision.caltech.edu/Image_Datasets/Caltech256/ For abnormal image detection, we use Abnormal 1001 as abnormal images : http://paul.rutgers.edu/~babaks/abnormality_detection.html Normal image classes are taken from PASCAL VOC dataset: http://host.robots.ox.ac.uk/pascal/VOC/voc2007/ Edit prototext files to reflect correct paths. Specifically, In solverVGG / solverdistance files, change \u0026lsquo;net\u0026rsquo; and \u0026lsquo;snapshot_prefix\u0026rsquo; with correct file paths. In VGGjoint2 / joint2 files, change \u0026lsquo;source\u0026rsquo; parameter in both data and data_c layers. Move distance_layer.py to caffe/python folder.\n  Training/ Testing Abnormal image detection Two sub directories \u0026lsquo;Abnormal_Object_Dataset\u0026rsquo; and \u0026lsquo;Normal_Object_Dataset\u0026rsquo; should exist in caffe/data. Each sub folder (of each class) should ne numbered started from 1.\nThere exists four modes of operation. To test just first class:\n Using Alexnet features $python examples/OneClass/src/src/run.py \u0026ndash;dataset data/ \u0026ndash;backbone Alex \u0026ndash;cafferoot /home/labuser/caffe/ \u0026ndash;nclass 6 \u0026ndash;noneclass 1 \u0026ndash;task abnormal \u0026ndash;type feature\n Using VGG16 features $python examples/OneClass/src/src/run.py \u0026ndash;dataset data/ \u0026ndash;backbone VGG \u0026ndash;cafferoot /home/labuser/caffe/ \u0026ndash;nclass 6 \u0026ndash;noneclass 1 \u0026ndash;task abnormal \u0026ndash;type feature\n Using Alexnet DOC (ours) $python examples/OneClass/src/src/run.py \u0026ndash;dataset data/ \u0026ndash;backbone Alex \u0026ndash;cafferoot /home/labuser/caffe/ \u0026ndash;nclass 6 \u0026ndash;noneclass 1 \u0026ndash;task abnormal\n Using VGG16 DOC (ours) $python examples/OneClass/src/src/run.py \u0026ndash;dataset data/ \u0026ndash;backbone VGG \u0026ndash;cafferoot /home/labuser/caffe/ \u0026ndash;nclass 6 \u0026ndash;noneclass 1 \u0026ndash;task abnormal\n  If all 6 classes needs to be tested replace \u0026ndash;noneclass 6.\nNovelty Detection Novelty detection dataset should be stored in the caffe/data/novelty directory. Each subfolder (of each class) should ne numbered started from 1.\nThere exists four modes of operation. To test just first class:\n Using Alexnet features $python examples/OneClass/src/src/run.py \u0026ndash;dataset data/ \u0026ndash;backbone Alex \u0026ndash;cafferoot /home/labuser/caffe/ \u0026ndash;nclass 6 \u0026ndash;noneclass 1 \u0026ndash;task novelty \u0026ndash;type feature\n Using VGG16 features $python examples/OneClass/src/src/run.py \u0026ndash;dataset data/ \u0026ndash;backbone VGG \u0026ndash;cafferoot /home/labuser/caffe/ \u0026ndash;nclass 6 \u0026ndash;noneclass 1 \u0026ndash;task novelty \u0026ndash;type feature\n Using Alexnet DOC (ours) $python examples/OneClass/src/src/run.py \u0026ndash;dataset data/ \u0026ndash;backbone Alex \u0026ndash;cafferoot /home/labuser/caffe/ \u0026ndash;nclass 6 \u0026ndash;noneclass 1 \u0026ndash;task novelty\n Using VGG16 DOC (ours) $python examples/OneClass/src/src/run.py \u0026ndash;dataset data/ \u0026ndash;backbone VGG \u0026ndash;cafferoot /home/labuser/caffe/ \u0026ndash;nclass 6 \u0026ndash;noneclass 1 \u0026ndash;task novelty\n  If 40 classes needs to be tested instead of just the first, replace \u0026ndash;noneclass 40.\nArguments 1.\u0026ndash;name : Name of the network. Used to name the performance curve plot and text output containing match scores.\n2.\u0026ndash;type : Type of CNN : oneclass / feature. When oneclass is used classification is done using DOC. Otherwise pre-trained deep features are used.\n3.\u0026ndash;output : Output directory name.\n4.\u0026ndash;dataset : Specify the path to the training dataset. Eg: data/abnormal/\n5.\u0026ndash;cafferoot : Specify the path to the caffe installation. Default is : /home/labuser/caffe/\n6.\u0026ndash;backbone : Specify the backbone: VGG/Alex\n7.\u0026ndash;nclass : Number of total classes in the dataset. 256 for novelty detection and 6 for abnormal image detection.\n8.\u0026ndash;noneclass : Number of classes to be considered for one-class testing. We used 40 for novelty detection. 6 for abnormal image detection.\n9.\u0026ndash;task : Specify oneclass task novelty/ abnormal\n10.\u0026ndash;niter : Number of training iterations\n11.\u0026ndash;visualize : True/ False specifies whether it is required to generate ROC curve plot.\noutput A text file with one-class score values will be written to the output folder. If \u0026lsquo;\u0026ndash;visualize\u0026rsquo; option is set to True, a ROC curve will also be generated.\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/imagenet/",
	"title": "ImageNet",
	"tags": [],
	"description": "",
	"content": " IMAGENET The IMAGENET dataset\nImageNet is a dataset of images that are organized according to the WordNet hierarchy. WordNet contains approximately 100,000 phrases and ImageNet has provided around 1000 images on average to illustrate each phrase.\nDataset Statistics Size 150 GB Number of Records: Total number of images: ~1,500,000; each with multiple bounding boxes and respective class labels\n* Total number of non-empty synsets: 21841 * Total number of images: 14,197,122 * Number of images with bounding box annotations: 1,034,908 * Number of synsets with SIFT features: 1000 * Number of images with SIFT features: 1.2 million  References  Imagenet  Samples Dataset Usage Download dataset http://www.image-net.org/\nTensorflow Start by cloning the TensorFlow models repo from GitHub. Run the following commands:\ngit clone https://github.com/tensorflow/models.git cd models/tutorials/image/imagenet python classify_image.py  If the model runs correctly, the script will produce the following output:\ngiant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.88493) indri, indris, Indri indri, Indri brevicaudatus (score = 0.00878) lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00317) custard apple (score = 0.00149) earthstar (score = 0.00127)  Torch\ngit clone https://github.com/soumith/imagenet-multiGPU.torch.git  Requirements  Install torch on a machine with CUDA GPU If on Mac OSX, run brew install coreutils findutils to get GNU versions of wc, find, and cut Download Imagenet-12 dataset from http://image-net.org/download-images . It has 1000 classes and 1.2 million images.  Data processing The images dont need to be preprocessed or packaged in any database. It is preferred to keep the dataset on an SSD but we have used the data loader comfortably over NFS without loss in speed. We just use a simple convention: SubFolderName == ClassName. So, for example: if you have classes {cat,dog}, cat images go into the folder dataset/cat and dog images go into dataset/dog\nThe training images for imagenet are already in appropriate subfolders (like n07579787, n07880968). You need to get the validation groundtruth and move the validation images into appropriate subfolders. To do this, download ILSVRC2012_img_train.tar ILSVRC2012_img_val.tar and use the following commands:\n# extract train data mkdir train \u0026amp;\u0026amp; mv ILSVRC2012_img_train.tar train/ \u0026amp;\u0026amp; cd train tar -xvf ILSVRC2012_img_train.tar \u0026amp;\u0026amp; rm -f ILSVRC2012_img_train.tar find . -name \u0026#34;*.tar\u0026#34; | while read NAME ; do mkdir -p \u0026#34;${NAME%.tar}\u0026#34;; tar -xvf \u0026#34;${NAME}\u0026#34; -C \u0026#34;${NAME%.tar}\u0026#34;; rm -f \u0026#34;${NAME}\u0026#34;; done # extract validation data cd ../ \u0026amp;\u0026amp; mkdir val \u0026amp;\u0026amp; mv ILSVRC2012_img_val.tar val/ \u0026amp;\u0026amp; cd val \u0026amp;\u0026amp; tar -xvf ILSVRC2012_img_val.tar wget -qO- https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh | bash Now you are all set!\nIf your imagenet dataset is on HDD or a slow SSD, run this command to resize all the images such that the smaller dimension is 256 and the aspect ratio is intact. This helps with loading the data from disk faster.\nfind . -name \u0026#34;*.JPEG\u0026#34; | xargs -I {} convert {} -resize \u0026#34;256^\u0026gt;\u0026#34; {} Running The training scripts come with several options which can be listed by running the script with the flag \u0026ndash;help\nth main.lua --help To run the training, simply run main.lua By default, the script runs 1-GPU AlexNet with the CuDNN backend and 2 data-loader threads.\nth main.lua -data [imagenet-folder with train and val folders] For 2-GPU model parallel AlexNet + CuDNN, you can run it this way:\nth main.lua -data [imagenet-folder with train and val folders] -nGPU 2 -backend cudnn -netType alexnet Similarly, you can switch the backends to \u0026lsquo;cunn\u0026rsquo; to use a different set of CUDA kernels.\nYou can also alternatively train OverFeat using this following command:\nth main.lua -data [imagenet-folder with train and val folders] -netType overfeat # multi-GPU overfeat (let\u0026#39;s say 2-GPU) th main.lua -data [imagenet-folder with train and val folders] -netType overfeat -nGPU 2 The training script prints the current Top-1 and Top-5 error as well as the objective loss at every mini-batch. We hard-coded a learning rate schedule so that AlexNet converges to an error of 42.5% at the end of 53 epochs.\nAt the end of every epoch, the model is saved to disk (as model_[xx].t7 where xx is the epoch number). You can reload this model into torch at any time using torch.load\nmodel = torch.load(\u0026#39;model_10.t7\u0026#39;) -- loading back a saved model Similarly, if you would like to test your model on a new image, you can use testHook from line 103 in donkey.lua to load your image, and send it through the model for predictions. For example:\ndofile(\u0026#39;donkey.lua\u0026#39;) img = testHook({loadSize}, \u0026#39;test.jpg\u0026#39;) model = torch.load(\u0026#39;model_10.t7\u0026#39;) if img:dim() == 3 then img = img:view(1, img:size(1), img:size(2), img:size(3)) end predictions = model:forward(img:cuda()) If you ever want to reuse this example, and debug your scripts, it is suggested to debug and develop in the single-threaded mode, so that stack traces are printed fully.\nth main.lua -nDonkeys 0 [...options...] Code Description  main.lua (~30 lines) - loads all other files, starts training. opts.lua (~50 lines) - all the command-line options and description data.lua (~60 lines) - contains the logic to create K threads for parallel data-loading. donkey.lua (~200 lines) - contains the data-loading logic and details. It is run by each data-loader thread. random image cropping, generating 10-crops etc. are in here. model.lua (~80 lines) - creates AlexNet model and criterion train.lua (~190 lines) - logic for training the network. we hard-code a learning rate + weight decay schedule that produces good results. test.lua (~120 lines) - logic for testing the network on validation set (including calculating top-1 and top-5 errors) dataset.lua (~430 lines) - a general purpose data loader, mostly derived from here: imagenetloader.torch. That repo has docs and more examples of using this loader.  Theano Dependencies  numpy Theano Pylearn2 PyCUDA zeromq hickle  How to run Prepare raw ImageNet data Download ImageNet dataset and unzip image files.\nPreprocess the data This involves shuffling training images, generating data batches, computing the mean image and generating label files.\nSteps  Set paths in the preprocessing/paths.yaml. Each path is described in this file. Run preprocessing/generate_data.sh, which will call 3 python scripts and do all the mentioned steps. It runs for about 1~2 days. For a quick trial of the code, run preprocessing/generate_toy_data.sh, which takes ~10 minutes and proceed.  preprocessing/lists.txt is a static file that lists what files should be created by running generate_data.sh.\nTrain AlexNet Set configurations config.yaml contains common configurations for both the 1-GPU and 2-GPU version.\nspec_1gpu.yaml and spec_2gpu.yaml contains different configurations for the 1-GPU and 2-GPU version respectively.\nIf you changed preprocessing/paths.yaml, make sure you change corresponding paths in config.yaml, spec_1gpu.yaml and spec_2gpu.yaml accordingly.\nStart training 1-GPU version, run:\nTHEANO_FLAGS=mode=FAST_RUN,floatX=float32 python train.py\n2-GPU version, run:\nTHEANO_FLAGS=mode=FAST_RUN,floatX=float32 python train_2gpu.py\nValidation error and loss values are stored as weights_dir/val_record.npy\nHere we do not set device to gpu in THEANO_FLAGS. Instead, users should control which GPU(s) to use in spec_1gpu.yaml and spec_2gpu.yaml.\nPretrained AlexNet Pretrained AlexNet weights and configurations can be found at pretrained/alexnet\nCaffe git clone https://github.com/yihui-he/resnet-imagenet-caffe.git  train ResNet on ImageNet with Caffe All models are trained on 4 GPUs with a minibatch size of 128. Testing is turned off during training due to memory limit(at least 12GB is require). The LMDB data is obtained from the official caffe imagenet tutorial\nTo train a network, use train.sh. For example, train resnet-50 with gpu 0,1,2,3:\n#set caffe path in train.sh mkdir resnet_50/logs mkdir resnet_50/snapshot ./train.sh 0,1,2,3 resnet_50 resnet_50_ For better training results, please install my Caffe fork, since the official Caffe ImageData layer doesn\u0026rsquo;t support original paper\u0026rsquo;s augmentation (resize shorter side to 256 then crop to 224x224). Use my 224x224 mean image bgr.binaryproto accordingly\nSee resnet_50/ResNet-50-test.prototxt ImageData layer for details\nresnet-50 use resnet_50/ResNet-50-test.prototxt for training and validation\n(new) We\u0026rsquo;ve release a 2X accelerated ResNet-50 caffemodel using channel-pruning\nresnet-32 This is a bottleneck architecture,\nSince there\u0026rsquo;s no strong data augmentation and 10-crop test in caffe, the results maybe a bit low.\ntest accuracy: accuracy@1 = 0.67892, accuracy@5 = 0.88164\ntraining loss for resnet-32 is shown below:\nthe trained model is provided in release\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/_footer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " \u0026copy; Georgia Institute of Technology "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/audioset_dataset/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " AudioSet The AudioSet dataset\nThe AudioSet dataset is a large-scale collection of human-labeled 10-second sound clips drawn from YouTube videos. There are 2,084,320 YouTube videos containing 527 labels.\nDataset Statistics Reference paper https://research.google.com/pubs/pub45857.html\nSamples Dataset Usage Info about Dataset AudioSet dataset for download in two formats:\nText (csv) files describing, for each segment, the YouTube video ID, start time, end time, and one or more labels.\n128-dimensional audio features extracted at 1Hz. The audio features were extracted using a VGG-inspired acoustic model described in Hershey et. al., trained on a preliminary version of YouTube-8M. The features were PCA-ed and quantized to be compatible with the audio features provided with YouTube-8M. They are stored as TensorFlow Record files.\nThe labels are taken from the AudioSet ontology which can be downloaded from our AudioSet GitHub repository ( https://github.com/audioset/ontology).\nThe dataset is made available by Google Inc. under a Creative Commons Attribution 4.0 International (CC BY 4.0) license, while the ontology is available under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\nDataset split The dataset is divided in three disjoint sets: a balanced evaluation set, a balanced training set, and an unbalanced training set. In the balanced evaluation and training sets, each class has the same number of examples. The unbalanced training set contains the remainder of annotated segments.\nhttp://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/eval_segments.csv contains 20,383 segments from distinct videos, providing at least 59 examples for each of the 527 sound classes that are used. Because of label co-occurrence, many classes have more examples.\nhttp://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/balanced_train_segments.csv contains 22,176 segments from distinct videos chosen with the same criteria: providing at least 59 examples per class with the fewest number of total segments.\nhttp://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/unbalanced_train_segments.csv contains 2,042,985 segments from distinct videos, representing the remainder of the dataset.\nEach csv file has a three-line header with each line starting with “#”, and with the first two lines indicating the creation time and general statistics. Each subsequent line has columns defined by the third header line\nThe total size of the features is 2.4 gigabytes. They are stored in 12,228 TensorFlow record files, sharded by the first two characters of the YouTube video ID, and packaged as a tar.gz file.\nThe labels are stored as integer indices. They are mapped to sound classes via class_labels_indices.csv. The first line defines the column names\nThe labels are stored as integer indices. They are mapped to sound classes via class_labels_indices.csv. The first line defines the column names: index,mid,display_name. Subsequent lines describe the mapping for each class. For example:0,/m/09x0r,\u0026ldquo;Speech\u0026rdquo;,which means that “labels” with value 0 indicate segments labeled with “Speech”.\nDownload Features To download the features, you have the following options:\nManually download the tar.gz file from one of (depending on region): storage.googleapis.com/us_audioset/youtube_corpus/v1/features/features.tar.gz storage.googleapis.com/eu_audioset/youtube_corpus/v1/features/features.tar.gz storage.googleapis.com/asia_audioset/youtube_corpus/v1/features/features.tar.gz\nUse gsutil rsync, with the command: gsutil rsync -d -r features gs://{region}_audioset/youtube_corpus/v1/features\nWhere {region} is one of “eu”, “us” or “asia”. For example: gsutil rsync -d -r features gs://us_audioset/youtube_corpus/v1/features\nYou can use the YouTube-8M (https://research.google.com/youtube8m/index.html) starter code to train models on the released features from both AudioSet as well as YouTube-8M(https://github.com/google/youtube-8m). The code can be found in the YouTube-8M GitHub repository.\nTheano Getting Datasets Get the datasets as described above\nMake sure you have the bleeding edge version of Theano, or run\npip install --upgrade --no-deps git+git://github.com/Theano/Theano.git  If you would like to work with your existing working environment, it should satisfy the following requirements:\nPython 3 and dependencies On Mac, can be installed with brew install python3 On Ubuntu/Debian, can be installed with apt-get install python3 Dependencies can be installed with pip install -r youtube-dl==2017.9.15 pafy==0.5.3.1 multiprocessing-logging==0.2.4 sox==1.3.0 sk-video==1.1.8 PySoundFile==0.9.0.post1 ffmpeg On Mac, can be installed with brew install ffmpeg On Ubuntu/Debian, can be installed with apt-get install ffmpeg sox On Mac, can be installed with brew install sox On Ubuntu/Debian, can be installed with apt-get install sox\nclone audiosetdl https://github.com/marl/audiosetdl.git Modules and scripts for downloading Google\u0026rsquo;s AudioSet dataset, a dataset of ~2.1 million annotated segments from YouTube videos\nSetup  Clone the repository onto your machine.\n If you would like to get started right away with a standalone (mini)conda, environment, run setup.sh in the project directory. This will install a local Anaconda environment in \u0026lt;PROJECT DIR\u0026gt;/bin/miniconda. You can find a python executable at \u0026lt;PROJECT DIR\u0026gt;/bin/miniconda/bin/python.\n Example: ./setup.sh  If you would like to work with your existing working environment, it should satisfy the following requirements:\n Python 3 and dependencies On Mac, can be installed with brew install python3 On Ubuntu/Debian, can be installed with apt-get install python3 Dependencies can be installed with pip install -r \u0026lt;PROJECT DIR\u0026gt;/requirements.txt ffmpeg On Mac, can be installed with brew install ffmpeg On Ubuntu/Debian, can be installed with apt-get install ffmpeg sox On Mac, can be installed with brew install sox On Ubuntu/Debian, can be installed with apt-get install sox   Running As a single script  Run python download_audioset.py  If you are using the local standalone conda installation, either activate the conda virtual environment, or use the python executable found in the local conda installation. The script will automatically download the scripts into your data directory if they do not exist and then start downloading the audio and video for all of the segments in parallel. You can tweak how the downloading and processing is done. For example,  URL/path to dataset subset files Audio/video format and codec Different strategies for obtaining video Number of multiprocessing pool workers used Path to logging  Run python download_audioset.py -h for a full list of arguments   SLURM This can be run as a batch of SLURM jobs\n Run download_subset_files.sh\n Sets up the data directory structure in the given folder (which will be created) and downloads the AudioSet subset files to that directory. If the --split \u0026lt;N\u0026gt; option is used, the script splits the files into N parts, which will have a suffix for a job ID, e.g. eval_segments.csv.01. Example: ./download_subset_files.sh --split 10 /home/user/audiosetdl/data  Use sbatch to run the audiosetdl-job-array.s job array script\n SLURM job array script that can be run by sbatch. Be sure to edit this to change the location of the repository ($SRCDIR) and to set the data directory ($DATADIR). Update any other configurations, such as email notifications and memory usage as it fits your use case. Example: sbatch --array=1-10 audiosetdl-job-array.s   Keras VGGish The initial AudioSet release included 128-dimensional embeddings of each AudioSet segment produced from a VGG-like audio classification model that was trained on a large YouTube dataset (a preliminary version of what later became YouTube-8M).\nGoogle provides a TensorFlow definition of this model, which they call VGGish, as well as supporting code to extract input features for the model from audio waveforms and to post-process the model embedding output into the same format as the released embedding features.\nInstallation VGGish depends on the following Python packages:\nnumpy scipy resampy tensorflow six\nThese are all easily installable via, e.g., pip install numpy (as in the example command sequence below).\nAny reasonably recent version of these packages should work. TensorFlow should be at least version 1.0. We have tested with Python 2.7.6 and 3.4.3 on an Ubuntu-like system with NumPy v1.13.1, SciPy v0.19.1, resampy v0.1.5, TensorFlow v1.2.1, and Six v1.10.0.\nVGGish also requires downloading two data files:\nVGGish model checkpoint, in TensorFlow checkpoint format. Embedding PCA parameters, in NumPy compressed archive format. After downloading these files into the same directory as this README, the installation can be tested by running python vggish_smoke_test.py which runs a known signal through the model and checks the output.\nDeeplearning4j VGG16 pretrained model. How to run the project:  IntelliJ IDE:\nThis is a maven project. It\u0026rsquo;s developed in IntelliJ. The project can be loaded and run in IntelliJ. When run in IntelliJ, under \u0026ldquo;Run\u0026rdquo;-\u0026gt;\u0026ldquo;Edit Configurations\u0026rdquo;, update following:  VM options: -Xms8g -Xmx8g Program arguments:    Cases where videos cannot be downloaded  Video removed User account deleted Not available in country Need to sign in to view Video no longer exists Copyright takedown  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/celeba_dataset/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Large-scale CelebFaces Attributes (CelebA) Dataset The CelebA dataset\nCelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations. The dataset can be employed as the training and test sets for the following computer vision tasks: face attribute recognition, face detection, and landmark (or facial part) localization.\nDataset Statistics  10,177 number of identities,\n 202,599 number of face images, and\n 5 landmark locations, 40 binary attributes annotations per image\n  References  S. Yang, P. Luo, C. C. Loy, and X. Tang, \u0026ldquo;From Facial Parts Responses to Face Detection: A Deep Learning Approach\u0026rdquo;, in IEEE International Conference on Computer Vision (ICCV), 2015  Samples Download dataset http://pan.baidu.com/s/1eSNpdRG\nNote  The CelebA dataset is available for non-commercial research purposes only.  All images of the CelebA dataset are obtained from the Internet which are not property of MMLAB, The Chinese University of Hong Kong. The MMLAB is not responsible for the content nor the meaning of these images.  You agree not to reproduce, duplicate, copy, sell, trade, resell or exploit for any commercial purposes, any portion of the images and any portion of derived data. You agree not to further copy, publish or distribute any portion of the CelebA dataset. Except, for internal use at a single site within the same organization it is allowed to make copies of the dataset. The MMLAB reserves the right to terminate your access to the CelebA dataset at any time. The face identities are released upon request for research purposes only. Please contact us for details.  Dataset Usage Development Environment  Ubuntu 14.04 LTS NVIDIA GTX 1080 ti cuda 8.0 Python 2.7.6 pytorch 0.1.12 torchvision 0.1.8 matplotlib 1.3.1 imageio 2.2.0 scipy 0.19.1  Pytorch GAN implemetation 1 Git clone https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN.git\n If you want to train using cropped CelebA dataset, you have to change isCrop = False to isCrop = True.  3.pytorch_CelebA_DCGAN.py requires 64 x 64 size image, so you have to resize CelebA dataset (celebA_data_preprocess.py).\npytorch_CelebA_DCGAN.py added learning rate decay code.\nTheano  Git clone https://github.com/tkarras/progressive_growing_of_gans.git  The repository contains a command-line tool for recreating bit-exact replicas of the HDF5 datasets that we used in the paper. The tool also provides various utilities for operating on HDF5 files:\nThe repository contains a command-line tool for recreating bit-exact replicas of the HDF5 datasets that we used in the paper. The tool also provides various utilities for operating on HDF5 files:\nusage: h5tool.py [-h]  \u0026hellip;\ninspect Print information about HDF5 dataset. compare Compare two HDF5 datasets. display Display images in HDF5 dataset. extract Extract images from HDF5 dataset. create_custom Create HDF5 dataset for custom images. create_mnist Create HDF5 dataset for MNIST. create_mnist_rgb Create HDF5 dataset for MNIST-RGB. create_cifar10 Create HDF5 dataset for CIFAR-10. create_lsun Create HDF5 dataset for single LSUN category. create_celeba Create HDF5 dataset for CelebA. create_celeba_hq Create HDF5 dataset for CelebA-HQ.  Type \u0026ldquo;h5tool.py  -h\u0026rdquo; for more information. The create_* commands take the original dataset as input and produce the corresponding HDF5 file as output. Additionally, the create_celeba_hq command requires a set of data files representing deltas from the original CelebA dataset. The deltas can be downloaded from Google Drive ( https://drive.google.com/open?id=0B4qLcYyJmiz0TXY1NG02bzZVRGs )(27.6GB)\nTensorFLow Git clone https://github.com/carpedm20/DCGAN-tensorflow.git\n$ python main.py \u0026ndash;dataset celebA \u0026ndash;input_height=108 \u0026ndash;train \u0026ndash;crop\nTo test with an existing model:\n $ python main.py --dataset celebA --input_height=108 --crop  $ mkdir data/DATASET_NAME \u0026hellip; add images to data/DATASET_NAME \u0026hellip; $ python main.py \u0026ndash;dataset celebA \u0026ndash;train $ python main.py \u0026ndash;dataset celebA ```\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/kinetics_datasets/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Kinetics The Kinetics dataset\nKinetics is a large-scale, high-quality dataset of YouTube video URLs which include a diverse range of human focused actions\nStatistics The dataset consists of approximately 300,000 video clips, and covers 400 human action classes with at least 400 video clips for each action class. Each clip lasts around 10s and is labeled with a single class. All of the clips have been through multiple rounds of human annotation, and each is taken from a unique YouTube video. The actions cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands and hugging\nReference https://arxiv.org/abs/1705.07750\nDataset The dataset is composed of three splits with corresponding CSV / JSON files:\nTraining set (9.2 MB zip file) Validation set (781 kB zip file) Test set (1.1 MB zip file, with annotations held out for the purpose of the ActivityNet challenge. Once the challenge is over, we plan to release the annotations.)\nIn the CSV files, each row describes one video and the columns are organized as follows:\nlabel - (string) a human-readable name for the action class. Characters used are lowercase letters, spaces, and single quotation (\u0026lsquo;). (Held out for the test set.) youtube_id - (string) the YouTube identifier of the video the segment was extracted from. One may view the selected video at http://youtu.be/${youtube_id}. time_start - (integer) the starting time of the action snippet in the video, in seconds. time_end - (integer) the ending time of the action snippet in the video, in seconds. split - (string) \u0026ldquo;train\u0026rdquo;, \u0026ldquo;val\u0026rdquo;, or \u0026ldquo;test\u0026rdquo;. is_cc - (bool) whether the video has a creative commons license, accessed on 2017-04-18. (Held out for the test set.)\nThe JSON files contain the same data as the CSV files, but formatted differently. Check the ActivityNet website for details at: http://activity-net.org/.\nThe validation and test sets each contain a maximum of 50 and 100 videos per class, respectively. However some classes may have less than the maximum, and over time YouTube videos may be deleted or taken down from public viewing by the uploading user. For the ActivityNet challenge, scoring will consider only those videos available after the submission deadline.\nIn some cases the video may end before time_end, but we always provide a 10-second window from time_start to time_end, so as much of the clip should be used as possible.\nSamples Tensorflow deeplearn-Kinetics-i3d:\nSetup First follow the instructions for installing Sonnet.\nThen, clone this repository using\n$ git clone https://github.com/deepmind/kinetics-i3d\nSample code Run the example code using\n$ python evaluate_sample.py\nTheano Getting Datasets Get the datasets as described above\nMake sure you have the bleeding edge version of Theano, or run\npip install --upgrade --no-deps git+git://github.com/Theano/Theano.git  If you would like to work with your existing working environment, it should satisfy the following requirements:\nPython 3 and dependencies On Mac, can be installed with brew install python3 On Ubuntu/Debian, can be installed with apt-get install python3 Dependencies can be installed with pip install -r youtube-dl==2017.9.15 pafy==0.5.3.1 multiprocessing-logging==0.2.4 sox==1.3.0 sk-video==1.1.8 PySoundFile==0.9.0.post1 ffmpeg On Mac, can be installed with brew install ffmpeg On Ubuntu/Debian, can be installed with apt-get install ffmpeg sox On Mac, can be installed with brew install sox On Ubuntu/Debian, can be installed with apt-get install sox\nSetup  Clone the repository https://github.com/marl/audiosetdl.git onto your machine.\n If you would like to get started right away with a standalone (mini)conda, environment, run setup.sh in the project directory. This will install a local Anaconda environment in \u0026lt;PROJECT DIR\u0026gt;/bin/miniconda. You can find a python executable at \u0026lt;PROJECT DIR\u0026gt;/bin/miniconda/bin/python.\n Example: ./setup.sh  If you would like to work with your existing working environment, it should satisfy the following requirements:\n Python 3 and dependencies On Mac, can be installed with brew install python3 On Ubuntu/Debian, can be installed with apt-get install python3 Dependencies can be installed with pip install -r \u0026lt;PROJECT DIR\u0026gt;/requirements.txt ffmpeg On Mac, can be installed with brew install ffmpeg On Ubuntu/Debian, can be installed with apt-get install ffmpeg sox On Mac, can be installed with brew install sox On Ubuntu/Debian, can be installed with apt-get install sox   Running kinetics/filter_subset.sh \u0026lt;filter_list\u0026gt; \u0026lt;kinetics_subset_csv\u0026gt; \u0026lt;output_file\u0026gt; will filter the given Kinetics subset csv file to contain only the classes in the given filter list, and put it in a format that is compatible with this script.\nKeras deeplearn-Kinetics-i3d:\nUsage python evaluate_sample.py\nor\n[For help] python evaluate_sample.py -h\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/lisa_traffic_sign_dataset/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " LISA Traffic Sign Dataset The LISA_Traffic_Sign Dataset\nThe LISA Traffic Sign Dataset is a set of videos and annotated frames containing US traffic signs. It is released in two stages, one with only the pictures and one with both pictures and videos. The images are available now, while the full dataset is underway and will be made available soon.\nDataset Statistics  47 US sign types 7855 annotations on 6610 frames. Sign sizes from 6x6 to 167x168 pixels. Images obtained from different cameras. Image sizes vary from 640x480 to 1024x522 pixels. Some images in color and some in grayscale. Full version of the dataset includes videos for all annotated signs. Each sign is annotated with sign type, position, size, occluded (yes/no), on side road (yes/no). All annotations are save in plain text .csv-files. Includes a set of Python tools to handle the annotations and easily extract relevant signs from the dataset.  Samples Dataset Usage TensorFlow Implementation of Single Shot MultiBox Detector in TensorFlow, to detect and classify traffic signs\nImplementation of Single Shot MultiBox Detector (SSD) in TensorFlow, to detect and classify traffic signs. This implementation was able to achieve 40-45 fps on a GTX 1080 with an Intel Core i7-6700K.\nNote this project is still work-in-progress. The main issue now is model overfitting.\nCurrently only stop signs and pedestrian crossing signs are detected. Example detection images are below.\nDependencies  Python 3.5+ TensorFlow v0.12.0 Pickle OpenCV-Python Matplotlib (optional)  How to run Clone this repository somewhere, let\u0026rsquo;s refer to it as $ROOT\nTo run predictions using the pre-trained model: * Download the pre-trained model to $ROOT * cd $ROOT * python inference.py -m demo * This will take the images from sample_images, annotate them, and display them on screen * To run predictions on your own images and/or videos, use the -i flag in inference.py (see the code for more details) * Note the model severly overfits at this time\nTraining the model from scratch: * Download the LISA Traffic Sign Dataset, and store it in a directory $LISA_DATA * cd $LISA_DATA * Follow instructions in the LISA Traffic Sign Dataset to create \u0026lsquo;mergedAnnotations.csv\u0026rsquo; such that only stop signs and pedestrian crossing signs are shown * cp $ROOT/data_gathering/create_pickle.py $LISA_DATA * python create_pickle.py * cd $ROOT * ln -s $LISA_DATA/resized_images_* . * ln -s $LISA_DATA/data_raw_*.p . * python data_prep.py * This performs box matching between ground-truth boxes and default boxes, and packages the data into a format used later in the pipeline * python train.py * This trains the SSD model * python inference.py -m demo\nDifferences between original SSD implementation Obivously, we are only detecting certain traffic signs in this implementation, whereas the original SSD implemetation detected a greater number of object classes in the PASCAL VOC and MS COCO datasets. Other notable differences are: * Uses AlexNet as the base network * Input image resolution is 400x260 * Uses a dynamic scaling factor based on the dimensions of the feature map relative to original image dimensions\nPerformance As mentioned above, this SSD implementation was able to achieve 40-45 fps on a GTX 1080 with an Intel Core i7 6700K.\nThe inference time is the sum of the neural network inference time, and Non-Maximum Suppression (NMS) time. Overall, the neural network inference time is significantly less than the NMS time, with the neural network inference time generally between 7-8 ms, whereas the NMS time is between 15-16 ms. The NMS algorithm implemented here has not been optimized, and runs on CPU only, so further effort to improve performance can be done there.\nDataset characteristics The entire LISA Traffic Sign Dataset consists of 47 distinct traffic sign classes. Since we are only concered with a subset of those classes, we only use a subset of the LISA dataset. Also, we ignore all training samples where we do not find a matching default box, further reducing our dataset\u0026rsquo;s size. Due to this process, we end up with very little data to work with.\nIn order to improve on this issue, we can perform image data augmentation, and/or pre-train the model on a larger dataset (e.g. VOC2012, ILSVRC)\nTraining process Given the small size of our pruned dataset, I chose a train/validation split of 95\u0026frasl;5. The model was trained with Adadelta optimizers, with the default parameters provided by TensorFlow. The model was trained over 200 epochs, with a batch size of 32.\nAreas of improvement There are multiple potential areas of improvement in this project:\n Pre-train the model on VOC2012 and/or ILSVRC Image data augmentation Hyper-parameter tuning Optimize NMS alogorithm, or leverage existing optimized NMS algorithm Implement and report mAP metric Try different base networks Expand to more traffic sign classes  Reference The LISA Traffic Sign Dataset and associated tools are released under academic license agreement. If you use the LISA Traffic Sign Database, please cite\nAndreas Møgelmose, Mohan M. Trivedi, and Thomas B. Moeslund, \u0026ldquo;Vision based Traffic Sign Detection and Analysis for Intelligent Driver Assistance Systems: Perspectives and Survey,\u0026rdquo; IEEE Transactions on Intelligent Transportation Systems, 2012.\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/vqa_datasets/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " VISUAL QA The VQA dataset\nVQA is a new dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer.\nDataset Statistics 265,016 images (COCO and abstract scenes) 1,105,904 questions 11,059,040 ground truth answers\nAt least 3 questions (5.4 questions on average) per image 10 ground truth answers per question 3 plausible (but likely incorrect) answers per question Automatic evaluation metric\nReferences  VQA: Visual Question Answering  Samples Dataset Usage Download dataset http://www.visualqa.org/download.html\nRequirements python 2.7 scikit-image (visit this page for installation) matplotlib (visit this page for installation)\nFiles ./Questions - For v2.0, download the question files from the VQA download page, extract them and place in this folder. - For v1.0, both real and abstract, question files can be found on the VQA v1 download page. - Question files from Beta v0.9 release (123,287 MSCOCO train and val images, 369,861 questions, 3,698,610 answers) can be found below - training question files - validation question files - Question files from Beta v0.1 release (10k MSCOCO images, 30k questions, 300k answers) can be found here.\n./Annotations - For v2.0, download the annotations files from the VQA download page, extract them and place in this folder. - For v1.0, for both real and abstract, annotation files can be found on the VQA v1 download page. - Annotation files from Beta v0.9 release (123,287 MSCOCO train and val images, 369,861 questions, 3,698,610 answers) can be found below - training annotation files - validation annotation files - Annotation files from Beta v0.1 release (10k MSCOCO images, 30k questions, 300k answers) can be found here.\n./Images - For real, create a directory with name mscoco inside this directory. For each of train, val and test, create directories with names train2014, val2014 and test2015 respectively inside mscoco directory, download respective images from MS COCO website and place them in respective folders. - For abstract, create a directory with name abstract_v002 inside this directory. For each of train, val and test, create directories with names train2015, val2015 and test2015 respectively inside abstract_v002 directory, download respective images from VQA download page and place them in respective folders.\n./PythonHelperTools - This directory contains the Python API to read and visualize the VQA dataset - vqaDemo.py (demo script) - vqaTools (API to read and visualize data)\n./PythonEvaluationTools - This directory contains the Python evaluation code - vqaEvalDemo.py (evaluation demo script) - vqaEvaluation (evaluation code)\n./Results - OpenEnded_mscoco_train2014_fake_results.json (an example of a fake results file for v1.0 to run the demo) - Visit VQA evaluation page for more details.\n./QuestionTypes - This directory contains the following lists of question types for both real and abstract questions (question types are unchanged from v1.0 to v2.0). In a list, if there are question types of length n+k and length n with the same first n words, then the question type of length n does not include questions that belong to the question type of length n+k. - mscoco_question_types.txt - abstract_v002_question_types.txt\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/coil100_datasets/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Columbia Object Image Library (COIL-100) The COIL-100 dataset\nThis is a database of gray-scale images of 100 objects. The objects were placed on a motorized turntable against a black background. The turntable was rotated through 360 degrees to vary object pose with respect to a fixed color camera. Images of the objects were taken at pose intervals of 5 degrees. This corresponds to 72 poses per object. The images were size normalized. The database, called Columbia Object Image Library (COIL-100), was used in a real-time 100 object recognition system\nDataset Statistics  Database of 7,200 color images of 100 objects (72 images per object). The objects have a wide variety of complex geometric and reflectance characteristics.  Samples Dataset Usage Getting the Code To get a local copy of the code, clone it using git:\ngit clone https://github.com/emersonloureiro/tensorflow-examples.git cd into coil-100 mkdir images/inception-images - this is where the pre-processed images for re-training will be kept ./preprocess.sh - this will pre-process all images under images/training, to a format that the tensorflow inception model accepts  ./train.sh PATH_TO_INCEPTION_V3_MODEL - you\u0026rsquo;ll need to download the Inception V3 model and use the path you saved in place of PATH_TO_INCEPTION_V3_MODEL. You can tune the training parameters (e.g., max iterations) by modifying the train.sh script After training, run ./eval to see the level of accuracy you achieved\nTORCH git clone https://github.com/jwyang/JULE.torch.git ``` $ curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash $ git clone https://github.com/torch/distro.git ~/torch --recursive $ cd ~/torch; $ ./install.sh # and enter \u0026quot;yes\u0026quot; at the end to modify your bashrc $ source ~/.bashrc ``` After installing torch, you may also need install some packages using LuaRocks: ``` $ luarocks install nn $ luarocks install image ``` It is preferred to run the code on GPU. Thus you need to install cunn: ``` $ luarocks install cunn ``` lua-knn. It is used to compute the distance between neighbor samples. Go into the folder, and then compile it with: ``` $ luarocks make ``` #### Train Model ``` $ th train.lua -dataset COIL-100 -eta 0.9 ``` Note that it runs on fast mode by default. You can change it to regular mode by setting \u0026quot;-use_fast 0\u0026quot;. In the above command, eta is the unfolding rate. For face dataset, we recommand 0.2, while for other datasets, it is set to 0.9 to save training time. During training, you will see the normalize mutual information (NMI) for the clustering results. You can train multiple models in parallel by: ```$ th train.lua -dataset COIL-100 -eta 0.9 -num_nets 5 ``` You can also get the clustering performance when using raw image data and random CNN by ```$ th train.lua -dataset COIL-100 -eta 0.9 -updateCNN 0 ``` You can also change other hyper parameters for model training, such as K_s, K_c, number of epochs in each partial unrolled period, etc.  CAFFE Dependencies CUDA. Install CUDA on your PC. I used CUDA 7.5, but it should also work to use new versions. Visual Studio. It is flexible to use various version of VS. I used VS2013 in my experiments. Third Party. Caffe depends on several third-party libraries, including hdf5, boost, gflag, opencv, glog, to name a few. happynear has provided the compiled libraties at [Caffe-Windows] (https://github.com/happynear/caffe-windows). Download those libraries and place them in the root folder, then add the ./3rdparty/bin folder to your environment variable PATH. Please ensure that these libraries has the same dependency on CUDA to your project. git clone https://github.com/jwyang/JULE-Caffe.git #### Steps to run the code Open ./buildVS2013/MainBuilder.sln using Visual Studio. Ideally, you will see 11 projects in one solution. Among them, you will mainly use caffelib and caffe_unsupervised to reproduce the results in our paper. However, the projects might crash because of different version of CUDA you are using. In this case, change the CUDA version in vcxproj file of each project.  Reference: 1.\u0026ldquo;Columbia Object Image Library (COIL-100),\u0026rdquo; S. A. Nene, S. K. Nayar and H. Murase, Technical Report CUCS-006-96, February 1996.\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/coil20/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Columbia Object Image Library (COIL-20) The COIL20 dataset\nThis is a database of gray-scale images of 20 objects. The objects were placed on motorized turntable.The turn table was rotated through 360 degrees to vary object pose with respect to fixed camera.Images of objects were taken at pose interval of 5 degrees.This corresponds to 72 images per object.\nDataset Statistics The database has two sets. The first set contains 720 unprocessed images of 10 objects. The second set contains 1440 size normalized images of 20 objects.  Samples Dataset Usage Tensorflow COIL 20: Getting the Code To get a local copy of the code, clone it using git:\ngit clone https://github.com/emersonloureiro/tensorflow-examples.git cd into coil-20 mkdir images/inception-images - this is where the pre-processed images for re-training will be kept ./preprocess.sh - this will pre-process all images under images/training, to a format that the tensorflow inception model accepts  ./train.sh PATH_TO_INCEPTION_V3_MODEL - you\u0026rsquo;ll need to download the Inception V3 model and use the path you saved in place of PATH_TO_INCEPTION_V3_MODEL. You can tune the training parameters (e.g., max iterations) by modifying the train.sh script After training, run ./eval to see the level of accuracy you achieved\nTORCH git clone https://github.com/jwyang/JULE.torch.git ``` $ curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash $ git clone https://github.com/torch/distro.git ~/torch --recursive $ cd ~/torch; $ ./install.sh # and enter \u0026quot;yes\u0026quot; at the end to modify your bashrc $ source ~/.bashrc ``` After installing torch, you may also need install some packages using LuaRocks: ``` $ luarocks install nn $ luarocks install image ``` It is preferred to run the code on GPU. Thus you need to install cunn: ``` $ luarocks install cunn ``` lua-knn. It is used to compute the distance between neighbor samples. Go into the folder, and then compile it with: ``` $ luarocks make ``` #### Train Model ``` $ th train.lua -dataset COIL-20 -eta 0.9 ``` Note that it runs on fast mode by default. You can change it to regular mode by setting \u0026quot;-use_fast 0\u0026quot;. In the above command, eta is the unfolding rate. For face dataset, we recommand 0.2, while for other datasets, it is set to 0.9 to save training time. During training, you will see the normalize mutual information (NMI) for the clustering results. You can train multiple models in parallel by: ```$ th train.lua -dataset COIL-20 -eta 0.9 -num_nets 5 ``` You can also get the clustering performance when using raw image data and random CNN by ```$ th train.lua -dataset COIL-20 -eta 0.9 -updateCNN 0 ``` You can also change other hyper parameters for model training, such as K_s, K_c, number of epochs in each partial unrolled period, etc.  CAFFE Dependencies CUDA. Install CUDA on your PC. I used CUDA 7.5, but it should also work to use new versions. Visual Studio. It is flexible to use various version of VS. I used VS2013 in my experiments. Third Party. Caffe depends on several third-party libraries, including hdf5, boost, gflag, opencv, glog, to name a few. happynear has provided the compiled libraties at [Caffe-Windows] (https://github.com/happynear/caffe-windows). Download those libraries and place them in the root folder, then add the ./3rdparty/bin folder to your environment variable PATH. Please ensure that these libraries has the same dependency on CUDA to your project. git clone https://github.com/jwyang/JULE-Caffe.git #### Steps to run the code Open ./buildVS2013/MainBuilder.sln using Visual Studio. Ideally, you will see 11 projects in one solution. Among them, you will mainly use caffelib and caffe_unsupervised to reproduce the results in our paper. However, the projects might crash because of different version of CUDA you are using. In this case, change the CUDA version in vcxproj file of each project.  Reference: 1.Columbia Object Image Library (COIL-20),\u0026rdquo; S. A. Nene, S. K. Nayar and H. Murase, Technical Report CUCS-005-96, February 1996.\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/labelme/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Labelme Labelme Train in Spain and test in the rest of the world dataset\nTry to recognize and segment as many object categories as you can. Training images correspond to outdoor pictures taken in different cities of Spain.\nDataset Statistics Training set: contains more than 1000 fully annotated images and around 2000 partially annotated images. Including partially annotated images allows algorithms to show if they are able to benefit from additional partially labeled images. As we try to build large datasets, it will be common to have many images that are only partially annotated, therefore, developing algorithms and training strategies that can cope with this issue will allow using large datasets without having to make the labor intensive effort of careful image annotation.  Test set: it only contains images that are fully labeled. The test set corresponds to images taken from the rest of the world which guarantees that images will be quite different between training and test.\nSamples Dataset Usage Tensorflow Download training set from here http://groups.csail.mit.edu/vision/LabelMe/Benchmarks/spain/training.tar.gz\nTORCH Download training set from here http://groups.csail.mit.edu/vision/LabelMe/Benchmarks/spain/training.tar.gz ``` $ curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash $ git clone https://github.com/torch/distro.git ~/torch --recursive $ cd ~/torch; $ ./install.sh # and enter \u0026quot;yes\u0026quot; at the end to modify your bashrc $ source ~/.bashrc ``` After installing torch, you may also need install some packages using LuaRocks: ``` $ luarocks install nn $ luarocks install image ``` It is preferred to run the code on GPU. Thus you need to install cunn: ``` $ luarocks install cunn ``` lua-knn. It is used to compute the distance between neighbor samples. Go into the folder, and then compile it with: ``` $ luarocks make ```  CAFFE Download training set from here http://groups.csail.mit.edu/vision/LabelMe/Benchmarks/spain/training.tar.gz\nDependencies CUDA. Install CUDA on your PC. I used CUDA 7.5, but it should also work to use new versions. Visual Studio. It is flexible to use various version of VS. I used VS2013 in my experiments. Third Party. Caffe depends on several third-party libraries, including hdf5, boost, gflag, opencv, glog, to name a few. happynear has provided the compiled libraties at [Caffe-Windows] (https://github.com/happynear/caffe-windows). Download those libraries and place them in the root folder, then add the ./3rdparty/bin folder to your environment variable PATH. Please ensure that these libraries has the same dependency on CUDA to your project. git clone https://github.com/jwyang/JULE-Caffe.git #### Steps to run the code Open ./buildVS2013/MainBuilder.sln using Visual Studio. Ideally, you will see 11 projects in one solution. Among them, you will mainly use caffelib and caffe_unsupervised to reproduce the results in our paper. However, the projects might crash because of different version of CUDA you are using. In this case, change the CUDA version in vcxproj file of each project.  Reference: 1.LabelMe: a database and web-based tool for image annotation . B. Russell, A. Torralba, K. Murphy, W. T. Freeman. International Journal of Computer Vision, 2007.\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/norb_dataset/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " NORB DATASET The Norb dataset\nThis database is intended for experiments in 3D object reocgnition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees)\nDataset Statistics The training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).  Samples Dataset Usage The \u0026ldquo;-dat\u0026rdquo; files store the image sequences. The \u0026ldquo;-cat\u0026rdquo; files store the corresponding category of the images. Each \u0026ldquo;-dat\u0026rdquo; file stores 29,160 image pairs (6 categories, 5 instances, 6 lightings, 9 elevations, and 18 azimuths). The 6-th category is for images without objects, which can be used to train a system to reject images as none of the 5 object categories. Each corresponding \u0026ldquo;-cat\u0026rdquo; file contains 29,160 category labels (0 for animal, 1 for human, 2 for plane, 3 for truck, 4 for car, 5 for blank).\nEach \u0026ldquo;-info\u0026rdquo; file stores 29,160 10-dimensional vectors, which contain additional information about the corresponding images. The first 4 elements in the vector are: - 1. the instance in the category (0 to 9) - 2. the elevation (0 to 8, which mean cameras are 30, 35,40,45,50,55,60,65,70 degrees from the horizontal respectively) - 3. the azimuth (0,2,4,\u0026hellip;,34, multiply by 10 to get the azimuth in degrees) - 4. the lighting condition (0 to 5) and the next 6 elements describe the peturbations added to the object when superposed onto a cluttered background. (see next section)\nFor regular training and testing, \u0026ldquo;-dat\u0026rdquo; and \u0026ldquo;-cat\u0026rdquo; files are sufficient. \u0026ldquo;-info\u0026rdquo; files are provided in case some other forms of classification or preprocessing are needed.\nTensorflow NORB: Getting the Code To get a local copy of the code, clone it using git:\ngit clone https://github.com/shashanktyagi/DC-GAN-on-NORB-dataset.git run dcgan_main.py  Keras To get a local copy of the code, clone it using git:\ngit clone https://github.com/ndrplz/small_norb.git Plug-and-play python wrapper around the small NORB dataset.  Torch Use the following code to load and run norb in torch https://github.com/rosejn/torch-datasets/blob/master/dataset/smallnorb.lua\nReference: 1.https://cs.nyu.edu/~ylclab/data/norb-v1.0/\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/openimages/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Open Images Dataset The Open Images dataset Open Images is a dataset of almost 9 million URLs for images. These images have been annotated with image-level labels bounding boxes spanning thousands of classes. The dataset contains a training set of 9,011,219 images, a validation set of 41,260 images and a test set of 125,436 images.\nDataset Statistics Size 500 GB (Compressed) Number of Records: 9,011,219 images with more than 5k labels References  Imagenet  Samples Dataset Usage git clone https://github.com/openimages/dataset.git cd tools use the scripts provided to classify the images based on pretrained models.  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/datasets/stl10_datset/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " STL10 The STL-10 dataset\nThe STL-10 dataset is an image recognition dataset for developing unsupervised feature learning, deep learning, self-taught learning algorithms. It is inspired by the CIFAR-10 dataset but with some modifications. In particular, each class has fewer labeled training examples than in CIFAR-10, but a very large set of unlabeled examples is provided to learn image models prior to supervised training.\nDataset Statistics 10 classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck. Images are 96x96 pixels, color. 500 training images (10 pre-defined folds), 800 test images per class. 100000 unlabeled images for unsupervised learning. These examples are extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set. Images were acquired from labeled examples on ImageNet.  Samples Dataset Usage There are three files: train.mat, test.mat and unlabeled.mat. These files contain variables: X, y: The matrix \u0026ldquo;X\u0026rdquo; contains the images for the file as a matrix with 1 example per row. In each row, the pixels are laid out in column-major order, one channel at a time. That is, the first 96*96 values are the red channel, the next 96*96 are green, and the last are blue. To convert these to a matrix of RGB images, use: reshape(X,10000,96,96,3). The vector \u0026ldquo;y\u0026rdquo; contains the labels in the range 1 to 10. class_names: Contains the text name of each class. fold_indices: In train.mat only. Contains the pre-selected indices of the examples to be used for the 10 training trials. For the i\u0026rsquo;th fold, use: X(fold_indices{i}, :) and y(fold_indices{i}) as your training set.\nThe binary files are split into data and label files with suffixes: train_X.bin, train_y.bin, test_X.bin and test_y.bin. Within each, the values are stored as tightly packed arrays of uint8\u0026rsquo;s. The images are stored in column-major order, one channel at a time. That is, the first 96*96 values are the red channel, the next 96*96 are green, and the last are blue. The labels are in the range 1 to 10. The unlabeled dataset, unlabeled.bin, is in the same format, but there is no \u0026ldquo;_y.bin\u0026rdquo; file. A class_names.txt file is included for reference, with one class name per line. The file fold_indices.txt contains the (zero-based) indices of the examples to be used for each training fold. The first line contains the indices for the first fold, the second line, the second fold, and so on.\nTheano ini_caltech101: Getting the Code To get a local copy of the code, clone it using git:\ngit clone https://github.com/mttk/STL10.git stl10_input.py contains all the necessary methods for downloading and reading data along with a test main funct for displaying an image.  Make sure you have the bleeding edge version of Theano, or run\npip install --upgrade --no-deps git+git://github.com/Theano/Theano.git  Keras To get a local copy of the code, clone it using git:\ngit clone https://github.com/mttk/STL10.git stl10_input.py contains all the necessary methods for downloading and reading data along with a test main funct for displaying an image. To classify images using keras:git clone https://github.com/rajatvikramsingh/stl10-vgg16.git Run vgg_transfer.py to run classify images of stl10 using keras  Reference: 1.Adam Coates, Honglak Lee, Andrew Y. Ng An Analysis of Single Layer Networks in Unsupervised Feature Learning AISTATS, 2011.\n"
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/frameworks/caffe/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Caffe The statistical info of Caffe is shown as follows:\n 678 text files. 677 unique files.\n 282 files ignored.  CLOC v 1.60 T=2.68 s (162.6 files/s, 36000.4 lines/s)    Language files blank comment code     C++ 188 4726 4888 41803   C/C++ Header 106 3341 7438 18325   Python 38 1431 3026 5232   CMake 33 435 476 1957   MATLAB 22 94 205 627   Bourne Shell 40 166 159 589   make 1 92 107 500   CSS 3 71 8 359   HTML 2 19 8 173   YAML 2 11 20 43   SUM: 435 10386 16335 69608    "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/frameworks/mxnet/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "2116 text files.\n2088 unique files.\n705 files ignored.\ncloc v 1.60 T=4.60 s (317.1 files/s, 71521.2 lines/s)\n   Language files blank comment code     Python 563 16447 35338 70836   C/C++ Header 258 6874 16657 58866   C++ 193 5318 6582 35875   Perl 79 3874 4689 21408   Scala 130 2683 5911 15865   Bourne Shell 107 820 2274 3014   Java 41 609 954 2175   Maven 17 67 13 1535   CMake 14 270 378 1489   CSS 1 237 47 1192   Javascript 10 121 168 1165   make 10 179 199 772   MATLAB 6 116 326 724   Cython 6 105 118 591   HTML 4 26 18 359   DOS Batch 4 85 81 270   YAML 8 51 60 261   C 1 20 28 201   XML 4 32 32 192   Bourne Again Shell 1 7 21 19   SUM: 1457 37941 73894 216809    "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/frameworks/tensorflow/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "LOC 0.4.1\n   Language Files Lines Blank Comment Code     Python 2309 729862 109417 65993 554452   C++ 2581 696781 90510 80917 525354   C/C++ Header 1372 209403 33251 57547 118605   Markdown 263 47881 10571 0 37310   Go 29 31742 1882 14027 15833   Java 85 16319 2209 3948 10162   Bourne Shell 112 11554 1595 3440 6519   Protobuf 94 9206 1463 3686 4057   Plain Text 41 3716 251 0 3465   Objective-C++ 18 2709 389 321 1999   C 9 1320 169 141 1010   Makefile 10 1407 178 266 963   XML 45 1429 253 445 731   Batch 10 276 57 0 219   Perl 2 227 36 41 150   JSON 4 126 0 0 126    LinkerScript 8 64 4 0 60   Objective-C 2 88 20 26 42   YAML 1 42 3 24 15   Autoconf 1 13 0 0 13   Total 6996 1764165 252258 230822 1281085    "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/frameworks/theano/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " LOC\nLanguage Files Lines Blank Comment Code Python 380 216122 34663 18566 162893 Plain Text 189 35241 9147 0 26094 C 51 26012 1903 5110 18999 TeX 4 3317 415 286 2616 JavaScript 10 2044 220 296 1528 HTML 4 404 32 0 372 CUDA 2 451 59 100 292 CUDA Header 1 433 84 75 274 CSS 4 254 40 0 214 C/C++ Header 5 141 25 25 91 YAML 1 66 6 0 60 reStructuredText 1 50 15 0 35 Batch 1 43 8 0 35 Makefile 3 35 9 3 23 Autoconf 1 15 0 0 15 Bourne Shell 1 21 4 2 15\nMarkdown 2 13 3 0 10 Total 660 284662 46633 24463 213566 "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/frameworks/torch/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "166 text files. 166 unique files.\n66 files ignored.\n[CLOC]()http://cloc.sourceforge.net) v 1.60 T=0.86 s (159.7 files/s, 41829.2 lines/s)\n   Language files blank comment code     C 50 2668 955 17270   Lua 21 1227 424 8701   C/C++ Header 51 465 281 2287   CMake 14 197 186 1409   YAML 2 0 0 83    "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/",
	"title": "GTDLBench",
	"tags": [],
	"description": "",
	"content": " GTDLBench With its effective and wide real applications, deep learning attracts great attention nowadays. Many deep learning frameworks emerged to facilitate deep learning research, development and applications. However, few research has been conducted to effectively measure these frameworks. And it still remains a critical task on choosing the optimal framework for specific deep learning models and applications. GTDLBench aims at introducing a hybrid benchmark to systematically measure the performance of these deep learning frameworks as well as the performance of different hardware devices. The performance metrics derived from both deep learning and systems field are introduced to present an easy and holistic approach for deep learning benchmarking.\nMetrics End-to-end Benchmark  Training Time (s) Testing Time (s) Accuracy (%)  Micro Benchmark  Latency (ms)  Tutorials  Benchmarking on MNIST Benchmarking on CIFAR-10 Benchmarking on CIFAR-100  Deep Learning Frameworks Frameworks\n Caffe MXNet TensorFlow Theano Torch  Datasets  AT\u0026amp;T Faces | Download Caltech-101 | Download CIFAR-10 | Download CIFAR-100 | Download MNIST | Download  Deep Learning Models  Default-on-MNIST Default-on-CIFAR-10  Please cite the following papers:\n  Papers under GTDLBench   Benchmarking Deep Learning Frameworks: Design Considerations, Metrics and Beyond    A Comparative Measurement Study of Deep Learning as a Service Framework    Experimental Characterizations and Analysis of Deep Learning Frameworks    @INPROCEEDINGS{GTDLBenchICDCS, author={{Liu}, Ling and {Wu}, Yanzhao and {Wei}, Wenqi and {Cao}, Wenqi and {Sahin}, Semih and {Zhang}, Qi}, booktitle={2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)}, title=\u0026quot;{Benchmarking Deep Learning Frameworks: Design Considerations, Metrics and Beyond}\u0026quot;, year={2018}, pages={1258-1269}, doi={10.1109/ICDCS.2018.00125}, ISSN={2575-8411}, month={July}, } @ARTICLE{GTDLBencharXiv, author = {{Wu}, Yanzhao and {Liu}, Ling and {Pu}, Calton and {Cao}, Wenqi and {Sahin}, Semih and {Wei}, Wenqi and {Zhang}, Qi}, title = \u0026quot;{A Comparative Measurement Study of Deep Learning as a Service Framework}\u0026quot;, journal = {arXiv e-prints}, keywords = {Computer Science - Performance, Computer Science - Machine Learning}, year = 2018, month = Oct, eid = {arXiv:1810.12210}, pages = {arXiv:1810.12210}, archivePrefix = {arXiv}, eprint = {1810.12210}, primaryClass = {cs.PF}, adsurl = {https://ui.adsabs.harvard.edu/\\#abs/2018arXiv181012210W}, adsnote = {Provided by the SAO/NASA Astrophysics Data System} } @INPROCEEDINGS{GTDLBenchBigData, author={{Wu}, Yanzhao and and {Cao}, Wenqi and {Sahin}, Semih and {Liu}, Ling}, booktitle={2018 IEEE 38th International Conference on Big Data}, title=\u0026quot;{Experimental Characterizations and Analysis of Deep Learning Frameworks}\u0026quot;, year={2018}, month={December}, }  "
},
{
	"uri": "http://YanzhaoWu.github.io/GTDLBench/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]