<!DOCTYPE html>
<html>
  <head>
    <title>GTDLBench</title>
    
      <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="revised" content="2019-01-05T14:27:35 EST">
<title> :: GTDLBench</title>
<link rel="shortcut icon" href="/GTDLBench/images/favicon.ico" type="image/x-icon" />
<link href="/GTDLBench/css/font-awesome.min.css" rel="stylesheet">
<link href="/GTDLBench/css/nucleus.css" rel="stylesheet">
<link href="/GTDLBench/theme-flex/style.css" rel="stylesheet">

<link rel="stylesheet" href="/GTDLBench/css/bootstrap.min.css">
<script src="/GTDLBench/js/jquery-2.x.min.js"></script>
<script type="text/javascript">
      var baseurl = "\/GTDLBench\/";
</script>
<meta name="description" content="">


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131741310-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-131741310-1');
</script>


    
  </head>
  <body data-url="/datasets/audioset_dataset/">
    
      <header>
  <div class="logo">
    
	
  
    <a class="baselink" href="/GTDLBench/">GTDLBench</a>
  


  </div>
  <div class="burger"><a href="javascript:void(0);" style="font-size:15px;">&#9776;</a></div>
    <nav class="shortcuts">
            <li class="" role="">
                <a href="https://github.com/YanzhaoWu/GTDLBench"  rel="noopener">
                  <i class='fa fa-github'></i> <label>Github Repo</label>
                </a>
            </li>
    </nav>
</header>
<article>
  <aside>
    <ul class="menu">
          <li data-nav-id="/" class="dd-item">
          <a href="/GTDLBench/">
            <i class="fa fa-fw fa-home"></i>
          </a>
          </li>
    <li data-nav-id="/datasets/" class="dd-item parent haschildren
        ">
      <div>
      <a href="/GTDLBench/datasets/">Datasets</a>
            <i class="fa fa-angle-down fa-lg category-icon"></i><i class="fa fa-circle-thin read-icon"></i>
      </div>
        <ul>
      <li data-nav-id="/datasets/mnist_datasets/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/mnist_datasets/">
            MNIST
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/cifar-10_datasets/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/cifar-10_datasets/">
            CIFAR-10
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/cifar-100_datasets/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/cifar-100_datasets/">
            CIFAR-100
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/att_face_dataset/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/att_face_dataset/">
            Faces (AT&amp;T)
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/caltech101_datasets/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/caltech101_datasets/">
            CALTECH101
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/caltech256_datasets/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/caltech256_datasets/">
            CALTECH256
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/imagenet/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/imagenet/">
            ImageNet
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/audioset_dataset/" class="dd-item active">
        <div>
          <a href="/GTDLBench/datasets/audioset_dataset/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/celeba_dataset/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/celeba_dataset/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/kinetics_datasets/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/kinetics_datasets/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/lisa_traffic_sign_dataset/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/lisa_traffic_sign_dataset/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/vqa_datasets/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/vqa_datasets/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/coil100_datasets/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/coil100_datasets/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/coil20/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/coil20/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/labelme/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/labelme/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/norb_dataset/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/norb_dataset/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/openimages/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/openimages/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/datasets/stl10_datset/" class="dd-item">
        <div>
          <a href="/GTDLBench/datasets/stl10_datset/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
        </ul>
    </li>
    <li data-nav-id="/frameworks/" class="dd-item haschildren
        ">
      <div>
      <a href="/GTDLBench/frameworks/">Frameworks</a><i class="fa fa-angle-right fa-lg category-icon"></i><i class="fa fa-circle-thin read-icon"></i>
      </div>
        <ul>
      <li data-nav-id="/frameworks/tensorflow/" class="dd-item">
        <div>
          <a href="/GTDLBench/frameworks/tensorflow/">
            TensorFlow
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/frameworks/caffe/" class="dd-item">
        <div>
          <a href="/GTDLBench/frameworks/caffe/">
            Caffe
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/frameworks/torch/" class="dd-item">
        <div>
          <a href="/GTDLBench/frameworks/torch/">
            Torch
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/frameworks/theano/" class="dd-item">
        <div>
          <a href="/GTDLBench/frameworks/theano/">
            Theano
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/frameworks/mxnet/" class="dd-item">
        <div>
          <a href="/GTDLBench/frameworks/mxnet/">
            
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
        </ul>
    </li>
    <li data-nav-id="/comparison/" class="dd-item haschildren
        ">
      <div>
      <a href="/GTDLBench/comparison/">Comparison of DL Frameworks</a><i class="fa fa-angle-right fa-lg category-icon"></i><i class="fa fa-circle-thin read-icon"></i>
      </div>
        <ul>
    <li data-nav-id="/comparison/tutorials/" class="dd-item haschildren
        ">
      <div>
      <a href="/GTDLBench/comparison/tutorials/">Dataset Used</a><i class="fa fa-angle-right fa-lg category-icon"></i><i class="fa fa-circle-thin read-icon"></i>
      </div>
        <ul>
      <li data-nav-id="/comparison/tutorials/benchmarking_on_mnist/" class="dd-item">
        <div>
          <a href="/GTDLBench/comparison/tutorials/benchmarking_on_mnist/">
            MNIST
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/comparison/tutorials/benchmarking_on_cifar10/" class="dd-item">
        <div>
          <a href="/GTDLBench/comparison/tutorials/benchmarking_on_cifar10/">
            CIFAR-10
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/comparison/tutorials/benchmarking_on_cifar100/" class="dd-item">
        <div>
          <a href="/GTDLBench/comparison/tutorials/benchmarking_on_cifar100/">
            CIFAR-100
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
        </ul>
    </li>
      <li data-nav-id="/comparison/mnist_comparison/" class="dd-item">
        <div>
          <a href="/GTDLBench/comparison/mnist_comparison/">
            Comparison on MNIST
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
      <li data-nav-id="/comparison/cifar10_comparison/" class="dd-item">
        <div>
          <a href="/GTDLBench/comparison/cifar10_comparison/">
            Comparison on CIFAR-10
          </a><i class="fa fa-circle-thin read-icon"></i>
        </div>
    </li>
        </ul>
    </li>
    <li data-nav-id="/info/" class="dd-item
        ">
      <div>
      <a href="/GTDLBench/info/">About</a><i class="fa fa-circle-thin read-icon"></i>
      </div>
    </li>




    </ul>
    <section>
    </section>
  </aside>
  <section class="page">
    
    <div class="nav-select">
      <center>Navigation : 
        <select onchange="javascript:location.href = this.value;">
          
    <option value="/datasets/" >
   Datasets</option> 
      <option value="/datasets/mnist_datasets/" >- MNIST</option>
      <option value="/datasets/cifar-10_datasets/" >- CIFAR-10</option>
      <option value="/datasets/cifar-100_datasets/" >- CIFAR-100</option>
      <option value="/datasets/att_face_dataset/" >- Faces (AT&amp;T)</option>
      <option value="/datasets/caltech101_datasets/" >- CALTECH101</option>
      <option value="/datasets/caltech256_datasets/" >- CALTECH256</option>
      <option value="/datasets/imagenet/" >- ImageNet</option>
      <option value="/datasets/audioset_dataset/"  selected>- </option>
      <option value="/datasets/celeba_dataset/" >- </option>
      <option value="/datasets/kinetics_datasets/" >- </option>
      <option value="/datasets/lisa_traffic_sign_dataset/" >- </option>
      <option value="/datasets/vqa_datasets/" >- </option>
      <option value="/datasets/coil100_datasets/" >- </option>
      <option value="/datasets/coil20/" >- </option>
      <option value="/datasets/labelme/" >- </option>
      <option value="/datasets/norb_dataset/" >- </option>
      <option value="/datasets/openimages/" >- </option>
      <option value="/datasets/stl10_datset/" >- </option>
  
    <option value="/frameworks/" >
   Frameworks</option>
    <option value="/comparison/" >
   Comparison of DL Frameworks</option>
    <option value="/info/" >
   About</option>



        </select>
      </center>
    </div>
      <div>
        <div class="searchbox">
          <input data-search-input id="search-by" type="text" placeholder="Search...">
        </div>
        <script type="text/javascript" src="/GTDLBench/js/lunr.min.js"></script>
        <script type="text/javascript" src="/GTDLBench/js/auto-complete.js"></script>
        <link href="/GTDLBench/css/auto-complete.css" rel="stylesheet">
        <script type="text/javascript">
          
              var baseurl = "\/GTDLBench\/";
          
        </script>
        <script type="text/javascript" src="/GTDLBench/js/search.js"></script>
      </div>
    

    <h1></h1>
    
    
    
    

<h1 id="audioset">AudioSet</h1>

<p><a href="https://research.google.com/audioset/dataset/index.html">The AudioSet dataset</a></p>

<p>The AudioSet dataset is a large-scale collection of human-labeled 10-second sound clips drawn from YouTube videos.
There are 2,084,320 YouTube videos containing 527 labels.</p>

<h2 id="dataset-statistics">Dataset Statistics</h2>

<p><img src="figures/audioset.PNG" alt="AudioSet Sample" /></p>

<h3 id="reference-paper">Reference paper</h3>

<p><a href="https://research.google.com/pubs/pub45857.html">https://research.google.com/pubs/pub45857.html</a></p>

<h3 id="samples">Samples</h3>

<p><img src="figures/audiosetsample.PNG" alt="AudioSet Sample" /></p>

<h2 id="dataset-usage">Dataset Usage</h2>

<h3 id="info-about-dataset">Info about Dataset</h3>

<p>AudioSet dataset for download in two formats:</p>

<p>Text (csv) files describing, for each segment, the YouTube video ID, start time, end time, and one or more labels.</p>

<p>128-dimensional audio features extracted at 1Hz. The audio features were extracted using a VGG-inspired acoustic model described in Hershey et. al., trained on a preliminary version of YouTube-8M. The features were PCA-ed and quantized to be compatible with the audio features provided with YouTube-8M. They are stored as TensorFlow Record files.</p>

<p>The labels are taken from the AudioSet ontology which can be downloaded from our AudioSet GitHub repository ( <a href="https://github.com/audioset/ontology">https://github.com/audioset/ontology</a>).</p>

<p>The dataset is made available by Google Inc. under a Creative Commons Attribution 4.0 International (CC BY 4.0) license, while the ontology is available under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.</p>

<h3 id="dataset-split">Dataset split</h3>

<p>The dataset is divided in three disjoint sets: a balanced evaluation set, a balanced training set, and an unbalanced training set. In the balanced evaluation and training sets,  each class has the same number of examples. The unbalanced training set contains the remainder of annotated segments.</p>

<p><a href="http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/eval_segments.csv">http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/eval_segments.csv</a> contains 20,383 segments from distinct videos, providing at least 59 examples for each of the 527 sound classes that are used. Because of label co-occurrence, many classes have more examples.</p>

<p><a href="http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/balanced_train_segments.csv">http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/balanced_train_segments.csv</a> contains 22,176 segments from distinct videos chosen with the same criteria: providing at least 59 examples per class with the fewest number of total segments.</p>

<p><a href="http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/unbalanced_train_segments.csv">http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/unbalanced_train_segments.csv</a> contains 2,042,985 segments from distinct videos, representing the remainder of the dataset.</p>

<p>Each csv file has a three-line header with each line starting with “#”, and with the first two lines indicating the creation time and general statistics. Each subsequent line has columns defined by the third header line</p>

<p>The total size of the features is 2.4 gigabytes. They are stored in 12,228 TensorFlow record files, sharded by the first two characters of the YouTube video ID, and packaged as a tar.gz file.</p>

<p>The labels are stored as integer indices. They are mapped to sound classes via class_labels_indices.csv. The first line defines the column names</p>

<p>The labels are stored as integer indices. They are mapped to sound classes via class_labels_indices.csv. The first line defines the column names: index,mid,display_name.
Subsequent lines describe the mapping for each class. For example:0,/m/09x0r,&ldquo;Speech&rdquo;,which means that “labels” with value 0 indicate segments labeled with “Speech”.</p>

<h4 id="download-features">Download Features</h4>

<p>To download the features, you have the following options:</p>

<p>Manually download the tar.gz file from one of (depending on region):
storage.googleapis.com/us_audioset/youtube_corpus/v1/features/features.tar.gz
storage.googleapis.com/eu_audioset/youtube_corpus/v1/features/features.tar.gz
storage.googleapis.com/asia_audioset/youtube_corpus/v1/features/features.tar.gz</p>

<p>Use gsutil rsync, with the command:
gsutil rsync -d -r features gs://{region}_audioset/youtube_corpus/v1/features</p>

<p>Where {region} is one of “eu”, “us” or “asia”. For example:
gsutil rsync -d -r features gs://us_audioset/youtube_corpus/v1/features</p>

<p>You can use the YouTube-8M (<a href="https://research.google.com/youtube8m/index.html">https://research.google.com/youtube8m/index.html</a>) starter code to train models on the released features from both AudioSet as well as YouTube-8M(<a href="https://github.com/google/youtube-8m">https://github.com/google/youtube-8m</a>). The code can be found in the YouTube-8M GitHub repository.</p>

<h3 id="theano">Theano</h3>

<h5 id="getting-datasets">Getting  Datasets</h5>

<p>Get the datasets as described above</p>

<p>Make sure you have the bleeding edge version of Theano, or run</p>

<pre><code>pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git
</code></pre>

<p>If you would like to work with your existing working environment, it should satisfy the following requirements:</p>

<p>Python 3 and dependencies
On Mac, can be installed with brew install python3
On Ubuntu/Debian, can be installed with apt-get install python3
Dependencies can be installed with pip install -r
        youtube-dl==2017.9.15
        pafy==0.5.3.1
        multiprocessing-logging==0.2.4
        sox==1.3.0
        sk-video==1.1.8
        PySoundFile==0.9.0.post1
ffmpeg
On Mac, can be installed with brew install ffmpeg
On Ubuntu/Debian, can be installed with apt-get install ffmpeg
sox
On Mac, can be installed with brew install sox
On Ubuntu/Debian, can be installed with apt-get install sox</p>

<p>clone audiosetdl <a href="https://github.com/marl/audiosetdl.git">https://github.com/marl/audiosetdl.git</a> Modules and scripts for downloading Google&rsquo;s AudioSet dataset, a dataset of ~2.1 million annotated segments from YouTube videos</p>

<h5 id="setup">Setup</h5>

<ul>
<li><p>Clone the repository onto your machine.</p></li>

<li><p>If you would like to get started right away with a standalone
<a href="https://conda.io/miniconda.html">(mini)conda</a>, environment, run <code>setup.sh</code>
in the project directory. This will install a local Anaconda environment in
<code>&lt;PROJECT DIR&gt;/bin/miniconda</code>. You can find a <code>python</code> executable at
<code>&lt;PROJECT DIR&gt;/bin/miniconda/bin/python</code>.</p>

<ul>
<li>Example: <code>./setup.sh</code></li>
</ul></li>

<li><p>If you would like to work with your existing working environment, it should
satisfy the following requirements:</p>

<ul>
<li><a href="https://www.python.org/downloads/">Python 3</a> and dependencies</li>
<li>On Mac, can be installed with <code>brew install python3</code></li>
<li>On Ubuntu/Debian, can be installed with <code>apt-get install python3</code></li>
<li>Dependencies can be installed with
<code>pip install -r &lt;PROJECT DIR&gt;/requirements.txt</code></li>
<li><a href="https://www.ffmpeg.org/"><code>ffmpeg</code></a></li>
<li>On Mac, can be installed with <code>brew install ffmpeg</code></li>
<li>On Ubuntu/Debian, can be installed with <code>apt-get install ffmpeg</code></li>
<li><a href="http://sox.sourceforge.net/"><code>sox</code></a></li>
<li>On Mac, can be installed with <code>brew install sox</code></li>
<li>On Ubuntu/Debian, can be installed with <code>apt-get install sox</code></li>
</ul></li>
</ul>

<h6 id="running">Running</h6>

<h5 id="as-a-single-script">As a single script</h5>

<ul>
<li>Run <code>python download_audioset.py</code>

<ul>
<li>If you are using the local standalone <code>conda</code> installation, either
activate the conda virtual environment, or use the python executable found
in the local conda installation.</li>
<li>The script will automatically download the scripts into your data
directory if they do not exist and then start downloading the audio and
video for all of the segments in parallel.</li>
<li>You can tweak how the downloading and processing is done. For example,

<ul>
<li>URL/path to dataset subset files</li>
<li>Audio/video format and codec</li>
<li>Different strategies for obtaining video</li>
<li>Number of multiprocessing pool workers used</li>
<li>Path to logging</li>
</ul></li>
<li>Run <code>python download_audioset.py -h</code> for a full list of arguments</li>
</ul></li>
</ul>

<h5 id="slurm">SLURM</h5>

<p>This can be run as a batch of SLURM jobs</p>

<ul>
<li><p>Run <code>download_subset_files.sh</code></p>

<ul>
<li>Sets up the data directory structure in the given folder (which will be
created) and downloads the AudioSet subset files to that directory.
If the <code>--split &lt;N&gt;</code> option is used, the script splits the files into N
parts, which will have a suffix for a job ID, e.g. <code>eval_segments.csv.01</code>.</li>
<li>Example: <code>./download_subset_files.sh --split 10 /home/user/audiosetdl/data</code></li>
</ul></li>

<li><p>Use <code>sbatch</code> to run the <code>audiosetdl-job-array.s</code> job array script</p>

<ul>
<li>SLURM job array script that can be run by sbatch. Be sure to edit this to
change the
location of the repository (<code>$SRCDIR</code>) and to set the data directory
(<code>$DATADIR</code>). Update any other configurations, such as email notifications
and memory usage as it fits your use case.</li>
<li>Example: <code>sbatch --array=1-10 audiosetdl-job-array.s</code></li>
</ul></li>
</ul>

<h3 id="keras">Keras</h3>

<h4 id="vggish">VGGish</h4>

<p>The initial AudioSet release included 128-dimensional embeddings of each AudioSet segment produced from a VGG-like audio classification model that was trained on a large YouTube dataset (a preliminary version of what later became YouTube-8M).</p>

<p>Google provides a TensorFlow definition of this model, which they call VGGish, as well as supporting code to extract input features for the model from audio waveforms and to post-process the model embedding output into the same format as the released embedding features.</p>

<p>Installation
VGGish depends on the following Python packages:</p>

<p>numpy
scipy
resampy
tensorflow
six</p>

<p>These are all easily installable via, e.g., pip install numpy (as in the example command sequence below).</p>

<p>Any reasonably recent version of these packages should work. TensorFlow should be at least version 1.0. We have tested with Python 2.7.6 and 3.4.3 on an Ubuntu-like system with NumPy v1.13.1, SciPy v0.19.1, resampy v0.1.5, TensorFlow v1.2.1, and Six v1.10.0.</p>

<p>VGGish also requires downloading two data files:</p>

<p>VGGish model checkpoint, in TensorFlow checkpoint format.
Embedding PCA parameters, in NumPy compressed archive format.
After downloading these files into the same directory as this README, the installation can be tested by running python vggish_smoke_test.py which runs a known signal through the model and checks the output.</p>

<h3 id="deeplearning4j">Deeplearning4j</h3>

<h4 id="vgg16-pretrained-model">VGG16 pretrained model.</h4>

<h5 id="how-to-run-the-project">How to run the project:</h5>

<ul>
<li>IntelliJ IDE:<br />
This is a maven project. It&rsquo;s developed in IntelliJ. The project can be loaded and run in IntelliJ.
When run in IntelliJ, under &ldquo;Run&rdquo;-&gt;&ldquo;Edit Configurations&rdquo;, update following:

<ul>
<li>VM options: -Xms8g -Xmx8g</li>
<li>Program arguments: <data path ie: C:\\Users\\tbhatia\\Desktop\\Research\\skymind\\data\\train></li>
</ul></li>
</ul>

<h2 id="cases-where-videos-cannot-be-downloaded">Cases where videos cannot be downloaded</h2>

<ul>
<li>Video removed</li>
<li>User account deleted</li>
<li>Not available in country</li>
<li>Need to sign in to view</li>
<li>Video no longer exists</li>
<li>Copyright takedown</li>
</ul>


    
    
        <div class="chevrons">
    <div id="navigation">
<a class="nav nav-prev" href="/GTDLBench/datasets/imagenet/" title="ImageNet"> <i class="fa fa-chevron-left"></i><label>ImageNet</label></a>
    <a class="nav nav-next" href="/GTDLBench/datasets/celeba_dataset/" title="" style="margin-right: 0px;"><label></label><i class="fa fa-chevron-right"></i></a></div>
  </div>

  </section>
</article>

<footer>

<div class="footline">
    

    

    

    
    <div class="github-link">
      <a href="https://github.com/YanzhaoWu/GTDLBench/edit/master/content/datasets/AudioSet_Dataset.md" target="blank"><i class="fa fa-code-fork"></i>
        Improve this page</a>
    </div>
    
  </div>


	<div>


  
    <p>
    &copy; Georgia Institute of Technology
</p>

  



	</div>
</footer>

<script src="/GTDLBench/js/clipboard.min.js"></script>

<link href="/GTDLBench/css/featherlight.min.css" rel="stylesheet">
<script src="/GTDLBench/js/featherlight.min.js"></script>



<script src="/GTDLBench/theme-flex/script.js"></script>


    

    
    

    
  </body>
</html>