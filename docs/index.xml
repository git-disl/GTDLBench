<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GTDLBench</title>
    <link>/GTDLBench/</link>
    <description>Recent content on GTDLBench</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/GTDLBench/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>MNIST</title>
      <link>/GTDLBench/comparison/tutorials/benchmarking_on_mnist/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/comparison/tutorials/benchmarking_on_mnist/</guid>
      <description>Benchmarking on MNIST: The following mentioned model definition files are under the folder: models/mnist/ .
Pre-setting: DLBENCH_ROOT=&amp;quot;path to the root directory of this benchmark&amp;quot;  TensorFlow: TensorFlow uses a variant of LeNet and its network structure is shown as follows:   TensorFlow default model   Run TensorFlow with its default MNIST setting:
cd $DLBENCH_ROOT/models/mnist/tensorflow/ python mnist_deep.py  The Training Time, Testing Time and Accuracy will appear after completion.</description>
    </item>
    
    <item>
      <title>CIFAR-10</title>
      <link>/GTDLBench/comparison/tutorials/benchmarking_on_cifar10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/comparison/tutorials/benchmarking_on_cifar10/</guid>
      <description>Benchmarking on CIFAR-10: The following mentioned model definition files are under the folder: models/cifar10/ .
Pre-setting: DLBENCH_ROOT=&amp;quot;path to the root directory of this benchmark&amp;quot;  TensorFlow: Run TensorFlow with its default MNIST setting:
cd $DLBENCH_ROOT/models/cifar10/tensorflow/ python cifar10_train.py &amp;gt; train_log.txt 2&amp;gt;&amp;amp;1  After the completion of training, run the following command to test the tranined model:
python cifar10_eval.py &amp;gt; test_log.txt 2&amp;gt;&amp;amp;1  The Accuracy will appear after completion of cifar10_eval.py. And the Training Time and Testing Time can be extracted from the train_log.</description>
    </item>
    
    <item>
      <title>CIFAR-100</title>
      <link>/GTDLBench/comparison/tutorials/benchmarking_on_cifar100/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/comparison/tutorials/benchmarking_on_cifar100/</guid>
      <description>Benchmarking on CIFAR-100: The following mentioned model definition files are under the folder: models/cifar100/ .
Pre-setting: DLBENCH_ROOT=&amp;quot;path to the root directory of this benchmark&amp;quot;  TensorFlow: Run TensorFlow with its default MNIST setting:
cd $DLBENCH_ROOT/models/cifar100/tensorflow/ python cifar100_train.py &amp;gt; train_log.txt 2&amp;gt;&amp;amp;1  After the completion of training, run the following command to test the tranined model:
python cifar100_eval.py &amp;gt; test_log.txt 2&amp;gt;&amp;amp;1  The Accuracy will appear after completion of cifar100_eval.py. And the Training Time and Testing Time can be extracted from the train_log.</description>
    </item>
    
    <item>
      <title>MNIST</title>
      <link>/GTDLBench/datasets/mnist_datasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/mnist_datasets/</guid>
      <description>MNIST Dataset The MNIST database of handwritten digits
Download Raw Dataset
Dataset Statistics  Color: Grey-scale Sample Size: 28x28  The number of categories of MNIST is 10, that is 0-9, 10 digits.
The Number of Samples per Category for MNIST    Category 0 1 2 3 4 5 6 7 8 9 Total     #Training Samples 5,923 6,742 5,958 6,131 5,842 5,421 5,918 6,265 5,851 5,949 60,000   #Testing Samples 980 1,135 1,032 1,010 982 892 958 1,028 974 1,009 10,000    Samples Dataset Usage MNIST in CSV The format is:</description>
    </item>
    
    <item>
      <title>CIFAR-10</title>
      <link>/GTDLBench/datasets/cifar-10_datasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/cifar-10_datasets/</guid>
      <description>CIFAR-10 dataset The CIFAR-10 dataset
Dataset Statistics  Color: RGB Sample Size: 32x32  The number of categories of CIFAR-10 is 10, that is airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.
The Number of Samples per Category for CIFAR-10    Category airplane automobile bird cat deer dog frog horse ship truck Total     #Training Samples 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 50,000    #Testing Samples 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 10,000    Samples Dataset Usage TensorFlow: TensorFlow @TensorFlow_Convolutional_Neural_Networks</description>
    </item>
    
    <item>
      <title>Comparison on MNIST</title>
      <link>/GTDLBench/comparison/mnist_comparison/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/comparison/mnist_comparison/</guid>
      <description>The benchmarking results are as following figure shows:
      google.charts.load(&#39;visualization&#39;, &#39;1&#39;, {&#39;package&#39;:[&#39;corechart&#39;, &#39;controls&#39;]}); function drawChart() { $.get(&#34;\/GTDLBench\/data\/MNISTServer1.csv&#34;, function(csvString) { var tmpData = $.csv.toArrays(csvString, {onParseValue: $.csv.hooks.castToScala}); var arrayData = []; var arrayData1 = []; var arrayData2 = []; for (var i = 0; i  </description>
    </item>
    
    <item>
      <title>TensorFlow</title>
      <link>/GTDLBench/frameworks/tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/frameworks/tensorflow/</guid>
      <description>TensorFlow is a popular open-source machine learning library developed by Google. It employs dataflow programming, and it is widely used for both research and production.
Install cd $DLBENCH_ROOT bash tensorflow-install.sh  This script will install TensorFlow through Miniconda. Please execute the following comannd to activate TensorFlow environment:
source activate tensorflow  Another recommended installation method is to install TensorFlow by compiling source codes.
The table below shows the statistics of TensorFlow.</description>
    </item>
    
    <item>
      <title>CIFAR-100</title>
      <link>/GTDLBench/datasets/cifar-100_datasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/cifar-100_datasets/</guid>
      <description>CIFAR-100 dataset The CIFAR-100 dataset
Dataset Statistics  Color: RGB Sample Size: 32x32  This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are roughly grouped into 20 superclasses. Each image comes with a &amp;ldquo;fine&amp;rdquo; label (the class to which it belongs) and a &amp;ldquo;coarse&amp;rdquo; label (the superclass to which it belongs).</description>
    </item>
    
    <item>
      <title>Caffe</title>
      <link>/GTDLBench/frameworks/caffe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/frameworks/caffe/</guid>
      <description>Caffe is a popular open-source deep learning library developed by Berkeley AI Research (BAIR) and by community contributors. It is designed to be fast, expressive and extensible.
Install cd $DLBENCH_ROOT bash caffe-install.sh  This script will install Caffe by compling it source codes.
The statistical information of Caffe is shown as follows:
 678 text files. 677 unique files.
 282 files ignored.  CLOC v 1.60 T=2.68 s (162.</description>
    </item>
    
    <item>
      <title>Comparison on CIFAR-10</title>
      <link>/GTDLBench/comparison/cifar10_comparison/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/comparison/cifar10_comparison/</guid>
      <description>The benchmarking results are as following figure shows:
      google.charts.load(&#39;visualization&#39;, &#39;1&#39;, {&#39;package&#39;:[&#39;corechart&#39;, &#39;controls&#39;]}); function drawChart() { $.get(&#34;\/GTDLBench\/data\/CIFAR10Server1.csv&#34;, function(csvString) { var tmpData = $.csv.toArrays(csvString, {onParseValue: $.csv.hooks.castToScala}); var arrayData = []; var arrayData1 = []; var arrayData2 = []; for (var i = 0; i  </description>
    </item>
    
    <item>
      <title>Faces (AT&amp;T)</title>
      <link>/GTDLBench/datasets/att_face_dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/att_face_dataset/</guid>
      <description>The Database of Faces (AT&amp;amp;T) The Database of Faces
Dataset Statistics  Color: Grey-scale Sample Size: 92x112 #Samples: 400 Dataset Size: 4.5 MB (compressed in .tar.z)  The original files are in PGM format, and can conveniently be viewed on UNIX &amp;trade; systems using the &amp;lsquo;xv&amp;rsquo; program. The size of each image is 92x112 pixels, with 256 grey levels per pixel. The images are organised in 40 directories (one for each subject), which have names of the form sX, where X indicates the subject number (between 1 and 40).</description>
    </item>
    
    <item>
      <title>Torch</title>
      <link>/GTDLBench/frameworks/torch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/frameworks/torch/</guid>
      <description>Torch is a scientific computing framework, supporting machine learning algorithms and GPUs. It is written in a scripting language, LuaJIT.
Install cd $DLBENCH_ROOT bash torch-install.sh  This script will install Torch by compiling its source codes. And its environment will be automatically configured.
The table below shows the statistics of Torch.
 166 text files 166 unique files
 66 files ignored  CLOC v 1.60 T=0.86 s (159.7 files/s, 41829.</description>
    </item>
    
    <item>
      <title>CALTECH101</title>
      <link>/GTDLBench/datasets/caltech101_datasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/caltech101_datasets/</guid>
      <description>CALTECH101 The CALTECH101 dataset
Caltech-101 contains a total of 9,146 images, split between 101 distinct object categories (faces, watches, ants, pianos, etc.) and a background category. This dataset contains 102 folders, the BACKGROUND_Google (the background category) can be removed, and users may use the left 101 categoies.
Dataset Statistics  Color: RGB Sample size: Roughtly 300x200 Dataset size: 1.2 GB  Overall, the dataset consists of pictures of objects belonging to 101 categories.</description>
    </item>
    
    <item>
      <title>Theano</title>
      <link>/GTDLBench/frameworks/theano/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/frameworks/theano/</guid>
      <description>Theano is a popular Python machine learning library, allowing users to define, optimize, and evaluate mathematical expressions. It has been powering intensive scientific workload, including deep learning since 2007.
Install cd $DLBENCH_ROOT bash theano-install.sh  This script will install Theano through Miniconda. Please execute the following comannd to activate Theano environment:
source activate theano  Another recommended installation method is to install Theano by compiling source codes.
The table below shows the statistics of Theano.</description>
    </item>
    
    <item>
      <title>CALTECH256</title>
      <link>/GTDLBench/datasets/caltech256_datasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/caltech256_datasets/</guid>
      <description>CALTECH256 The CALTECH256 dataset
Dataset Statistics  Color: RGB Sample Size:  Camprison with Caltech-101: The Number of Samples per Category for Caltech-256 Samples Dataset Usage TensorFlow: Git clone https://github.com/yukunchen113/ResnetCNN.git cd RestNetCNN   Use caltech256_bin.py to convet caltech256 images to tfrecord files for faster reading. Use caltech256_input.py input functions to convert input iput functions to batch images and labels. Model.py contains resnet model. Train.py trains the model Evals.py evaluates the model.</description>
    </item>
    
    <item>
      <title>ImageNet</title>
      <link>/GTDLBench/datasets/imagenet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/imagenet/</guid>
      <description>IMAGENET The IMAGENET dataset
ImageNet is a dataset of images that are organized according to the WordNet hierarchy. WordNet contains approximately 100,000 phrases and ImageNet has provided around 1000 images on average to illustrate each phrase.
Dataset Statistics Size 150 GB Number of Records: Total number of images: ~1,500,000; each with multiple bounding boxes and respective class labels
* Total number of non-empty synsets: 21841 * Total number of images: 14,197,122 * Number of images with bounding box annotations: 1,034,908 * Number of synsets with SIFT features: 1000 * Number of images with SIFT features: 1.</description>
    </item>
    
    <item>
      <title>LISA Traffic Sign</title>
      <link>/GTDLBench/datasets/lisa_traffic_sign_dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/lisa_traffic_sign_dataset/</guid>
      <description>LISA Traffic Sign Dataset The LISA_Traffic_Sign Dataset
The LISA Traffic Sign Dataset is a set of videos and annotated frames containing US traffic signs. It is released in two stages, one with only the pictures and one with both pictures and videos. The images are available now, while the full dataset is underway and will be made available soon.
Dataset Statistics  47 US sign types 7855 annotations on 6610 frames.</description>
    </item>
    
    <item>
      <title>USPS Dataset</title>
      <link>/GTDLBench/datasets/usps_dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/usps_dataset/</guid>
      <description>USPS Dataset A database for handwritten text recognition research.
Download Raw Dataset USPS Dataset
USPS Testing Dataset
Dataset Statistics  # of classes: 10 # of data: 7291 / 2007 (testing) # of features: 256  Samples Dataset Usage TensorFlow: https://github.com/darshanbagul/USPS_Digit_Classification
USPS_Digit_Classification Requirements  tensorflow==1.0.1 matplotlib==1.5.1 cv2==2.4.13 numpy==1.13.0 cPickle==1.71  Implementation 1. Logistic Regression 2. Single layered Neural Networks 3. Convolutional Neural Networks  Results  Logistic Regression
Accuracy on USPS data - 36.</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/_footer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/_footer/</guid>
      <description> &amp;copy; [DiSL](https://www.cc.gatech.edu/projects/disl/) @ Georgia Institute of Technology </description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/audioset_dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/audioset_dataset/</guid>
      <description>AudioSet The AudioSet dataset
The AudioSet dataset is a large-scale collection of human-labeled 10-second sound clips drawn from YouTube videos. There are 2,084,320 YouTube videos containing 527 labels.
Dataset Statistics Reference paper https://research.google.com/pubs/pub45857.html
Samples Dataset Usage Info about Dataset AudioSet dataset for download in two formats:
Text (csv) files describing, for each segment, the YouTube video ID, start time, end time, and one or more labels.
128-dimensional audio features extracted at 1Hz.</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/celeba_dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/celeba_dataset/</guid>
      <description>Large-scale CelebFaces Attributes (CelebA) Dataset The CelebA dataset
CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations. The dataset can be employed as the training and test sets for the following computer vision tasks: face attribute recognition, face detection, and landmark (or facial part) localization.</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/kinetics_datasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/kinetics_datasets/</guid>
      <description>Kinetics The Kinetics dataset
Kinetics is a large-scale, high-quality dataset of YouTube video URLs which include a diverse range of human focused actions
Statistics The dataset consists of approximately 300,000 video clips, and covers 400 human action classes with at least 400 video clips for each action class. Each clip lasts around 10s and is labeled with a single class. All of the clips have been through multiple rounds of human annotation, and each is taken from a unique YouTube video.</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/vqa_datasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/vqa_datasets/</guid>
      <description>VISUAL QA The VQA dataset
VQA is a new dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense knowledge to answer.
Dataset Statistics 265,016 images (COCO and abstract scenes) 1,105,904 questions 11,059,040 ground truth answers
At least 3 questions (5.4 questions on average) per image 10 ground truth answers per question 3 plausible (but likely incorrect) answers per question Automatic evaluation metric</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/coil100_datasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/coil100_datasets/</guid>
      <description>Columbia Object Image Library (COIL-100) The COIL-100 dataset
This is a database of gray-scale images of 100 objects. The objects were placed on a motorized turntable against a black background. The turntable was rotated through 360 degrees to vary object pose with respect to a fixed color camera. Images of the objects were taken at pose intervals of 5 degrees. This corresponds to 72 poses per object. The images were size normalized.</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/coil20/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/coil20/</guid>
      <description>Columbia Object Image Library (COIL-20) The COIL20 dataset
This is a database of gray-scale images of 20 objects. The objects were placed on motorized turntable.The turn table was rotated through 360 degrees to vary object pose with respect to fixed camera.Images of objects were taken at pose interval of 5 degrees.This corresponds to 72 images per object.
Dataset Statistics The database has two sets. The first set contains 720 unprocessed images of 10 objects.</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/labelme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/labelme/</guid>
      <description>Labelme Labelme Train in Spain and test in the rest of the world dataset
Try to recognize and segment as many object categories as you can. Training images correspond to outdoor pictures taken in different cities of Spain.
Dataset Statistics Training set: contains more than 1000 fully annotated images and around 2000 partially annotated images. Including partially annotated images allows algorithms to show if they are able to benefit from additional partially labeled images.</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/norb_dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/norb_dataset/</guid>
      <description>NORB DATASET The Norb dataset
This database is intended for experiments in 3D object reocgnition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees)
Dataset Statistics The training set is composed of 5 instances of each category (instances 4, 6, 7, 8 and 9), and the test set of the remaining 5 instances (instances 0, 1, 2, 3, and 5).</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/openimages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/openimages/</guid>
      <description>Open Images Dataset The Open Images dataset Open Images is a dataset of almost 9 million URLs for images. These images have been annotated with image-level labels bounding boxes spanning thousands of classes. The dataset contains a training set of 9,011,219 images, a validation set of 41,260 images and a test set of 125,436 images.
Dataset Statistics Size 500 GB (Compressed) Number of Records: 9,011,219 images with more than 5k labels References  Imagenet  Samples Dataset Usage git clone https://github.</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/datasets/stl10_datset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/datasets/stl10_datset/</guid>
      <description>STL10 The STL-10 dataset
The STL-10 dataset is an image recognition dataset for developing unsupervised feature learning, deep learning, self-taught learning algorithms. It is inspired by the CIFAR-10 dataset but with some modifications. In particular, each class has fewer labeled training examples than in CIFAR-10, but a very large set of unlabeled examples is provided to learn image models prior to supervised training.
Dataset Statistics 10 classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.</description>
    </item>
    
    <item>
      <title></title>
      <link>/GTDLBench/frameworks/mxnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/GTDLBench/frameworks/mxnet/</guid>
      <description>2116 text files.
2088 unique files.
705 files ignored.
cloc v 1.60 T=4.60 s (317.1 files/s, 71521.2 lines/s)
   Language files blank comment code     Python 563 16447 35338 70836   C/C++ Header 258 6874 16657 58866   C++ 193 5318 6582 35875   Perl 79 3874 4689 21408   Scala 130 2683 5911 15865   Bourne Shell 107 820 2274 3014   Java 41 609 954 2175   Maven 17 67 13 1535   CMake 14 270 378 1489   CSS 1 237 47 1192   Javascript 10 121 168 1165   make 10 179 199 772   MATLAB 6 116 326 724   Cython 6 105 118 591   HTML 4 26 18 359   DOS Batch 4 85 81 270   YAML 8 51 60 261   C 1 20 28 201   XML 4 32 32 192   Bourne Again Shell 1 7 21 19   SUM: 1457 37941 73894 216809    </description>
    </item>
    
  </channel>
</rss>